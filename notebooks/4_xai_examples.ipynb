{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d118a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Notebook: 4_xai_examples.ipynb\n",
    "#\n",
    "\n",
    "import ibbi\n",
    "\n",
    "# --- 1. Setup ---\n",
    "# Let's use the multi-class species classifier for our explanations.\n",
    "model = ibbi.create_model(\"species_classifier\", pretrained=True)\n",
    "explainer = ibbi.Explainer(model)\n",
    "\n",
    "# Load the test dataset and select an image to explain.\n",
    "print(\"Loading the test dataset...\")\n",
    "test_dataset = ibbi.get_dataset()\n",
    "image_to_explain = test_dataset[1010][\"image\"]  # Use an image from the test dataset\n",
    "print(\"Dataset loaded and an image has been selected for explanation.\")\n",
    "\n",
    "\n",
    "# --- 2. Explainability with LIME ---\n",
    "# LIME (Local Interpretable Model-agnostic Explanations) is great for a quick\n",
    "# visual explanation of which parts of the image contributed to a prediction.\n",
    "print(\"\\n--- Generating LIME Explanation ---\")\n",
    "# This returns the explanation object and the original image.\n",
    "lime_explanation, original_image = explainer.with_lime(image_to_explain)\n",
    "\n",
    "# You can then plot the explanation.\n",
    "# This will show the original image and then an overlay highlighting important regions.\n",
    "ibbi.plot_lime_explanation(lime_explanation, original_image, top_k=1)\n",
    "\n",
    "\n",
    "# --- 3. Explainability with SHAP ---\n",
    "# SHAP (SHapley Additive exPlanations) is more computationally intensive but provides\n",
    "# more robust, theoretically grounded explanations.\n",
    "print(\"\\n--- Generating SHAP Explanation ---\")\n",
    "\n",
    "# SHAP requires a background dataset to represent the \"absence\" of features.\n",
    "# `ibbi` provides a default one you can load.\n",
    "background_dataset = ibbi.get_shap_background_dataset()\n",
    "\n",
    "# We also need a small dataset of images to explain. For this example,\n",
    "# we'll just use our single image from the test dataset.\n",
    "explain_dataset = [{\"image\": image_to_explain}]\n",
    "\n",
    "# Generate the SHAP values. This is the most time-consuming step.\n",
    "# `max_evals` controls the number of model evaluations. Higher is more accurate but slower.\n",
    "shap_values = explainer.with_shap(\n",
    "    explain_dataset=explain_dataset,\n",
    "    background_dataset=background_dataset,\n",
    "    num_explain_samples=1,  # We are only explaining one image\n",
    "    max_evals=500,\n",
    ")\n",
    "\n",
    "# Plot the explanation for the first (and only) image in our set.\n",
    "# This will generate plots for the top predicted classes.\n",
    "ibbi.plot_shap_explanation(shap_values[0], model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IBBI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

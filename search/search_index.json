{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Intelligent Bark Beetle Identifier (IBBI)","text":"<p>IBBI is a Python package that provides a simple and unified interface for detecting and classifying bark and ambrosia beetles from images using state-of-the-art computer vision models.</p> <p>This package is designed to support entomological research by automating the laborious task of beetle identification, enabling high-throughput data analysis for ecological studies, pest management, and biodiversity monitoring.</p>"},{"location":"#the-need-for-automation","title":"The Need for Automation","text":"<p>The accurate and timely identification of bark and ambrosia beetle species is fundamental to forest health monitoring. However, many species are morphologically similar, making identification a significant bottleneck that requires highly specialized taxonomic expertise. IBBI addresses this challenge by providing an accessible, programmatic solution that automates the identification process.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Simple API: Access powerful detection and classification models with a single function call: <code>ibbi.create_model()</code>.</li> <li>Multiple Model Types:<ul> <li>Single-Class Detection: Detect the presence of any beetle in an image.</li> <li>Multi-Class Classification: Identify the species of a beetle from a cropped image.</li> <li>Zero-Shot Detection: Detect beetles using a text prompt, without prior training on that specific class.</li> </ul> </li> <li>Pre-trained Models: Leverages pre-trained models hosted on the Hugging Face Hub for immediate use.</li> <li>Model Explainability: Understand model predictions using SHAP (SHapley Additive exPlanations) to visualize which parts of an image contribute to the identification.</li> <li>Extensible: Designed to easily incorporate new model architectures in the future.</li> <li>Research-Focused: Aims to accelerate ecological research by automating beetle identification.</li> </ul>"},{"location":"#workflow-how-the-models-were-built","title":"Workflow: How the Models Were Built","text":"<p>The models in <code>ibbi</code> are trained using a detailed workflow, from data collection and annotation to model evaluation. This process ensures high-quality, reliable models for your research.</p> <p></p>"},{"location":"#package-api-and-usage","title":"Package API and Usage","text":"<p>The <code>ibbi</code> package is designed for ease of use. The main functions, inputs, and outputs are summarized below.</p> <p></p> <p>Ready to get started? Check out the Usage Guide.</p>"},{"location":"CONTRIBUTING/","title":"Contributing to IBBI","text":"<p>First off, thank you for considering contributing to IBBI! Your help is greatly appreciated. This document provides guidelines for contributing to the project.</p>"},{"location":"CONTRIBUTING/#seeking-support","title":"Seeking Support","text":"<p>If you have a general question about how to use <code>ibbi</code>, are not sure about a feature, or are encountering a bug, the best way to get help is by opening an issue on our GitHub Issue Tracker.</p> <p>This is the preferred method for getting support, as it allows the community and maintainers to track and respond to your query in one central place.</p>"},{"location":"CONTRIBUTING/#how-can-i-contribute","title":"How Can I Contribute?","text":"<p>There are many ways to contribute, from writing tutorials to implementing new models. Here are a few ideas:</p> <ul> <li>Reporting Bugs: If you find a bug, please open an issue on our GitHub issue tracker. Describe the issue in detail, including steps to reproduce it.</li> <li>Suggesting Enhancements: Have an idea for a new feature or an improvement to an existing one? Open an issue to start a discussion.</li> <li>Writing Documentation: Good documentation is key. If you find parts of our docs unclear or want to add a new tutorial, please let us know or submit a pull request.</li> <li>Adding New Models: If you have trained a new model that would be a good fit for IBBI, we'd love to hear about it.</li> <li>Submitting Pull Requests: If you've fixed a bug or implemented a new feature, you can submit a pull request.</li> </ul>"},{"location":"CONTRIBUTING/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":"<p>To get started with development, please follow these steps.</p> <ol> <li>Clone the repository:     This downloads the project source code to your local machine.</li> </ol> <pre><code>git clone https://github.com/ChristopherMarais/ibbi.git\ncd ibbi\n</code></pre> <ol> <li>Create a Conda environment:     We recommend using Conda to manage your Python environment to avoid conflicts with other projects. This command creates an environment named <code>ibbi</code> with Python 3.11.</li> </ol> <pre><code>conda env create -f environment.yml\nconda activate IBBI\n</code></pre> <ol> <li> <p>Install dependencies with Poetry:     This project uses Poetry for dependency management. The <code>environment.yml</code> file sets up Python and pip, and then we use Poetry to install the project dependencies.</p> </li> <li> <p>Install dependencies with Poetry:     This project uses Poetry for dependency management. These commands will install all the necessary packages for running and developing <code>ibbi</code>.</p> </li> </ol> <pre><code># Install PyTorch first, as its installation can be system-specific (CPU/GPU)\n# See https://pytorch.org/get-started/locally/ for the correct command\npip install torch torchvision torchaudio\n\n# Configure Poetry to use the existing Conda environment\npoetry config virtualenvs.create false --local\n\n# Install all other project dependencies, including development tools\npoetry install --with dev\n</code></pre> <ol> <li>Set up pre-commit hooks:     We use <code>pre-commit</code> to automatically run code formatters and linters before each commit. This ensures code quality and a consistent style across the project.</li> </ol> <pre><code>pre-commit install\n</code></pre> <p>The hooks will now run automatically every time you make a commit.</p>"},{"location":"CONTRIBUTING/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create a new branch for your feature or bug fix (e.g., <code>git checkout -b feature/my-new-feature</code>).</li> <li>Make your changes and commit them. Make sure your commit messages are clear and descriptive.</li> <li>Ensure all tests pass and that the pre-commit hooks run without errors.</li> <li>Push your branch to your fork on GitHub.</li> <li>Open a pull request from your branch to the <code>main</code> branch of the IBBI repository.</li> <li>In the pull request description, clearly describe the changes you've made and why. If it fixes an existing issue, please reference it (e.g., \"Fixes #123\").</li> </ol> <p>Thank you again for your interest in contributing!</p>"},{"location":"LICENSE/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2025 Chris</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"api/","title":"API Reference","text":"<p>This section provides an auto-generated API reference for the <code>ibbi</code> package.</p>"},{"location":"api/#ibbi","title":"<code>ibbi</code>","text":"<p>Main initialization file for the ibbi package.</p> <p>This file serves as the primary entry point for the <code>ibbi</code> library. It exposes the most important high-level functions and classes, making them directly accessible to the user under the <code>ibbi</code> namespace. This includes the core model creation factory (<code>create_model</code>), the main workflow classes (<code>Evaluator</code>, <code>Explainer</code>), and key utility functions for accessing datasets and managing the cache.</p> <p>The goal of this top-level <code>__init__.py</code> is to provide a clean and intuitive API, simplifying the user experience by abstracting away the underlying module structure.</p>"},{"location":"api/#ibbi.ModelType","title":"<code>ModelType = TypeVar('ModelType', YOLOSingleClassBeetleDetector, RTDETRSingleClassBeetleDetector, YOLOBeetleMultiClassDetector, RTDETRBeetleMultiClassDetector, GroundingDINOModel, YOLOWorldModel, UntrainedFeatureExtractor, HuggingFaceFeatureExtractor)</code>  <code>module-attribute</code>","text":"<p>A generic TypeVar for representing any of the model wrapper classes in the ibbi package.</p> <p>This is used for type hinting in functions and methods that can accept or return any of the available model types, providing flexibility while maintaining static type safety.</p>"},{"location":"api/#ibbi.Evaluator","title":"<code>Evaluator</code>","text":"<p>A unified evaluator for assessing IBBI models on various tasks.</p> <p>This class provides a streamlined interface for evaluating the performance of models on tasks such as object classification and embedding quality. It handles the boilerplate code for iterating through datasets, making predictions, and calculating a comprehensive suite of metrics for a holistic model assessment.</p> <p>The <code>Evaluator</code> is initialized with a model instance from the <code>ibbi</code> package. It provides methods to run different types of evaluations, returning detailed performance reports.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>ModelType</code> <p>The instantiated <code>ibbi</code> model to be evaluated.</p> Source code in <code>src\\ibbi\\evaluate\\__init__.py</code> <pre><code>class Evaluator:\n    \"\"\"A unified evaluator for assessing IBBI models on various tasks.\n\n    This class provides a streamlined interface for evaluating the performance of\n    models on tasks such as object classification and embedding quality.\n    It handles the boilerplate code for iterating through datasets, making predictions,\n    and calculating a comprehensive suite of metrics for a holistic model assessment.\n\n    The `Evaluator` is initialized with a model instance from the `ibbi` package.\n    It provides methods to run different types of evaluations, returning detailed\n    performance reports.\n\n    Attributes:\n        model (ModelType): The instantiated `ibbi` model to be evaluated.\n    \"\"\"\n\n    def __init__(self, model: ModelType):\n        \"\"\"Initializes the Evaluator with a specific model.\n\n        Args:\n            model (ModelType): The model to be evaluated. This should be an instance of a class\n                               that adheres to the `ModelType` protocol, meaning it has `predict`\n                               and `extract_features` methods.\n        \"\"\"\n        self.model = model\n\n    def object_classification(\n        self, dataset, iou_thresholds: Union[float, list[float]] = 0.5, predict_kwargs: Optional[dict[str, Any]] = None, **kwargs\n    ):\n        \"\"\"Runs a comprehensive object detection and classification performance analysis.\n\n        This method assesses the model's ability to both accurately localize and correctly\n        classify objects within a dataset. It iterates through the provided dataset, gathering\n        ground truth information and generating model predictions. These are then passed to the\n        `object_classification_performance` function to compute a detailed suite of metrics.\n\n        The evaluation provides a holistic view of performance, combining traditional object\n        detection metrics (like mAP) with a full suite of classification metrics for each IoU\n        threshold.\n\n        Args:\n            dataset (iterable): An iterable dataset where each item is a dictionary-like object\n                                containing at least an 'image' key. For evaluation, items should\n                                also contain an 'objects' key, which is a dictionary with 'bbox'\n                                and 'category' keys.\n            iou_thresholds (Union[float, list[float]], optional): The IoU threshold(s) at which\n                to compute mAP and classification metrics. Can be a single float or a list of floats.\n                Defaults to 0.5.\n            predict_kwargs (Optional[dict[str, Any]], optional): A dictionary of keyword arguments\n                to be passed directly to the model's `predict` method during evaluation.\n                This is useful for model-specific parameters like `text_prompt` for zero-shot models.\n                Defaults to None.\n            **kwargs: Additional keyword arguments to be passed to the underlying\n                      `object_classification_performance` function (e.g., `average`, `zero_division`).\n\n        Returns:\n            dict: A dictionary containing a comprehensive set of object detection and\n                  classification metrics, including per-iou threshold classification performance,\n                  and a detailed object-level performance table.\n        \"\"\"\n        if predict_kwargs is None:\n            predict_kwargs = {}\n\n        # Set classes for GroundingDINO before evaluation\n        if isinstance(self.model, GroundingDINOModel):\n            if \"text_prompt\" in predict_kwargs:\n                self.model.set_classes(predict_kwargs[\"text_prompt\"])\n\n        print(\"Running object classification evaluation...\")\n\n        if isinstance(self.model, (HuggingFaceFeatureExtractor, UntrainedFeatureExtractor)):\n            print(\"Warning: Object classification evaluation is not supported for pure feature extractors.\")\n            return {}\n\n        if isinstance(self.model, (GroundingDINOModel, YOLOWorldModel)):\n            if \"text_prompt\" not in predict_kwargs and not self.model.get_classes():\n                print(\"Warning: Zero-shot model has no classes set. Please provide a 'text_prompt' in 'predict_kwargs'.\")\n                return {}\n\n        gt_boxes, gt_labels, gt_image_ids, gt_label_names = [], [], [], []\n        pred_results_with_probs = []  # Full prediction result per image\n        # Initialize model_classes before the loop.\n        model_classes: list[str] = []\n        if isinstance(self.model, (GroundingDINOModel)):\n            if hasattr(self.model, \"get_classes\") and callable(self.model.get_classes):\n                raw_model_classes = self.model.get_classes()\n                if isinstance(raw_model_classes, dict):\n                    model_classes = list(raw_model_classes.values())\n                else:\n                    model_classes = raw_model_classes\n        class_name_to_idx: dict[str, int] = {}\n        idx_to_name: dict[int, str] = {}\n\n        print(\"Extracting ground truth and making predictions...\")\n        predict_kwargs_for_call = {**predict_kwargs, \"include_full_probabilities\": True}\n\n        for i, item in enumerate(tqdm(dataset)):\n            # Make the first prediction to set classes for YOLOWorld\n            results = self.model.predict(item[\"image\"], verbose=False, **predict_kwargs_for_call)\n            pred_results_with_probs.append(results)\n\n            if not model_classes:\n                if not hasattr(self.model, \"get_classes\") or not callable(self.model.get_classes):\n                    print(\"Warning: Model does not have a 'get_classes' method for class mapping. Skipping evaluation.\")\n                    return {}\n\n                raw_model_classes = self.model.get_classes()\n                if isinstance(raw_model_classes, dict):\n                    model_classes: list[str] = list(raw_model_classes.values())\n                else:\n                    model_classes: list[str] = raw_model_classes\n\n                if not model_classes:\n                    print(\"Warning: Model returned an empty class list. Cannot proceed with classification-dependent metrics.\")\n                    return {}\n\n                class_name_to_idx = {v: k for k, v in enumerate(model_classes)}\n                idx_to_name = dict(enumerate(model_classes))\n\n            # --- Extract Ground Truth ---\n            if \"objects\" in item and \"bbox\" in item[\"objects\"] and \"category\" in item[\"objects\"]:\n                for j in range(len(item[\"objects\"][\"category\"])):\n                    label_name = item[\"objects\"][\"category\"][j]\n                    gt_label_names.append(label_name)\n                    bbox = item[\"objects\"][\"bbox\"][j]\n                    x1, y1, w, h = bbox\n                    x2 = x1 + w\n                    y2 = y1 + h\n                    gt_boxes.append([x1, y1, x2, y2])\n                    gt_labels.append(class_name_to_idx.get(label_name, -1))\n\n                    gt_image_ids.append(i)\n\n        # The GT and raw prediction data is prepared. Now run the core evaluation logic.\n        performance_results = object_classification_performance(\n            np.array(gt_boxes),\n            gt_labels,\n            gt_image_ids,\n            pred_results_with_probs,\n            gt_label_names=gt_label_names,\n            iou_thresholds=iou_thresholds,\n            model_classes=model_classes,\n            idx_to_name=idx_to_name,\n            **kwargs,\n        )\n\n        # Apply naming to the mAP results\n        if \"per_class_AP_at_last_iou\" in performance_results:\n            class_aps = performance_results[\"per_class_AP_at_last_iou\"]\n            named_class_aps = {idx_to_name.get(class_id, class_id): ap for class_id, ap in class_aps.items()}\n            performance_results[\"per_class_AP_at_last_iou\"] = named_class_aps\n\n        return performance_results\n\n    def embeddings(\n        self,\n        dataset,\n        evaluation_level: str = \"image\",\n        use_umap: bool = True,\n        extract_kwargs: Optional[dict[str, Any]] = None,\n        batch_size: int = 32,\n        **kwargs,\n    ):\n        \"\"\"Evaluates the quality of the model's feature embeddings.\n\n        This method extracts feature embeddings from the provided dataset. It can operate\n        at two levels: 'image' (extracting one embedding per image) or 'object'\n        (extracting an embedding for each annotated object in each image). The quality of\n        these embeddings is then assessed using clustering algorithms and a suite of\n        internal and external validation metrics.\n\n        Args:\n            dataset (iterable): An iterable dataset where each item contains an 'image' key.\n                                For 'object' level evaluation, items should also contain 'objects'\n                                with 'bbox' and 'category' keys.\n            evaluation_level (str, optional): The level at which to evaluate embeddings.\n                                              Can be \"image\" or \"object\". Defaults to \"image\".\n            use_umap (bool, optional): If True, applies UMAP for dimensionality reduction\n                                       before clustering. Defaults to True.\n            extract_kwargs (Optional[dict[str, Any]], optional): Keyword arguments to be passed\n                to the model's `extract_features` method. Defaults to None.\n            batch_size (int, optional): The batch size for GPU distance matrix calculation.\n                                        Defaults to 32.\n            **kwargs: Additional keyword arguments to be passed to the `EmbeddingEvaluator`.\n                      See `ibbi.evaluate.embeddings.EmbeddingEvaluator` for more details.\n\n        Returns:\n            dict: A dictionary containing the results of the embedding evaluation, including\n                  clustering metrics and optionally, correlation with external data.\n        \"\"\"\n        if extract_kwargs is None:\n            extract_kwargs = {}\n        if evaluation_level not in [\"image\", \"object\"]:\n            raise ValueError(\"evaluation_level must be either 'image' or 'object'.\")\n\n        print(f\"Extracting embeddings for evaluation at the '{evaluation_level}' level...\")\n        embeddings_list = []\n        true_labels = []\n        valid_indices = []\n\n        # Pre-calculate label mappings for efficiency\n        unique_labels_lst = list(set(cat for item in dataset for cat in item.get(\"objects\", {}).get(\"category\", [])))\n        unique_labels = sorted(unique_labels_lst)\n        name_to_idx = {name: i for i, name in enumerate(unique_labels)}\n        idx_to_name = dict(enumerate(unique_labels))\n\n        for i, item in enumerate(tqdm(dataset)):\n            if evaluation_level == \"image\":\n                embedding = self.model.extract_features(item[\"image\"], **extract_kwargs)\n                if embedding is not None:\n                    embeddings_list.append(embedding)\n                    if \"objects\" in item and \"category\" in item[\"objects\"] and item[\"objects\"][\"category\"]:\n                        label_name = item[\"objects\"][\"category\"][0]\n                        if label_name in name_to_idx:\n                            true_labels.append(name_to_idx[label_name])\n                            valid_indices.append(len(embeddings_list) - 1)\n\n            elif evaluation_level == \"object\":\n                if \"objects\" not in item or \"bbox\" not in item[\"objects\"] or \"category\" not in item[\"objects\"]:\n                    continue\n\n                original_image = item[\"image\"]\n                for j, bbox in enumerate(item[\"objects\"][\"bbox\"]):\n                    x, y, w, h = bbox\n                    if w &gt; 0 and h &gt; 0:\n                        cropped_image = original_image.crop((x, y, x + w, y + h))\n                        embedding = self.model.extract_features(cropped_image, **extract_kwargs)\n                        if embedding is not None:\n                            embeddings_list.append(embedding)\n                            label_name = item[\"objects\"][\"category\"][j]\n                            if label_name in name_to_idx:\n                                true_labels.append(name_to_idx[label_name])\n                                valid_indices.append(len(embeddings_list) - 1)\n\n        if not embeddings_list:\n            print(\"Warning: Could not extract any valid embeddings from the dataset.\")\n            return {}\n\n        embeddings = np.array([emb.cpu().numpy().flatten() for emb in embeddings_list])\n        evaluator = EmbeddingEvaluator(embeddings, use_umap=use_umap, **kwargs)\n\n        results = {}\n        results[\"internal_cluster_validation\"] = evaluator.evaluate_cluster_structure()\n\n        if true_labels:\n            true_labels = np.array(true_labels)\n            results[\"external_cluster_validation\"] = evaluator.evaluate_against_truth(true_labels)\n            results[\"sample_results\"] = evaluator.get_sample_results(true_labels, label_map=idx_to_name)\n\n            try:\n                if len(np.unique(true_labels)) &gt;= 3:\n                    valid_embeddings = embeddings[valid_indices]\n                    evaluator_for_mantel = EmbeddingEvaluator(valid_embeddings, use_umap=False)\n                    mantel_corr, p_val, n, per_class_df = evaluator_for_mantel.compare_to_distance_matrix(\n                        true_labels, label_map=idx_to_name, batch_size=batch_size\n                    )\n                    results[\"mantel_correlation\"] = {\"r\": mantel_corr, \"p_value\": p_val, \"n_items\": n}\n                    results[\"per_class_centroids\"] = per_class_df\n                else:\n                    print(\"Not enough unique labels in the dataset subset to run the Mantel test.\")\n            except (ImportError, FileNotFoundError, ValueError) as e:\n                print(f\"Could not run Mantel test: {e}\")\n        else:\n            print(\"Dataset does not have the required 'objects' and 'category' fields for external validation.\")\n            results[\"sample_results\"] = evaluator.get_sample_results()\n\n        return results\n</code></pre>"},{"location":"api/#ibbi.Evaluator.__init__","title":"<code>__init__(model)</code>","text":"<p>Initializes the Evaluator with a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelType</code> <p>The model to be evaluated. This should be an instance of a class                that adheres to the <code>ModelType</code> protocol, meaning it has <code>predict</code>                and <code>extract_features</code> methods.</p> required Source code in <code>src\\ibbi\\evaluate\\__init__.py</code> <pre><code>def __init__(self, model: ModelType):\n    \"\"\"Initializes the Evaluator with a specific model.\n\n    Args:\n        model (ModelType): The model to be evaluated. This should be an instance of a class\n                           that adheres to the `ModelType` protocol, meaning it has `predict`\n                           and `extract_features` methods.\n    \"\"\"\n    self.model = model\n</code></pre>"},{"location":"api/#ibbi.Evaluator.embeddings","title":"<code>embeddings(dataset, evaluation_level='image', use_umap=True, extract_kwargs=None, batch_size=32, **kwargs)</code>","text":"<p>Evaluates the quality of the model's feature embeddings.</p> <p>This method extracts feature embeddings from the provided dataset. It can operate at two levels: 'image' (extracting one embedding per image) or 'object' (extracting an embedding for each annotated object in each image). The quality of these embeddings is then assessed using clustering algorithms and a suite of internal and external validation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>iterable</code> <p>An iterable dataset where each item contains an 'image' key.                 For 'object' level evaluation, items should also contain 'objects'                 with 'bbox' and 'category' keys.</p> required <code>evaluation_level</code> <code>str</code> <p>The level at which to evaluate embeddings.                               Can be \"image\" or \"object\". Defaults to \"image\".</p> <code>'image'</code> <code>use_umap</code> <code>bool</code> <p>If True, applies UMAP for dimensionality reduction                        before clustering. Defaults to True.</p> <code>True</code> <code>extract_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Keyword arguments to be passed to the model's <code>extract_features</code> method. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size for GPU distance matrix calculation.                         Defaults to 32.</p> <code>32</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to the <code>EmbeddingEvaluator</code>.       See <code>ibbi.evaluate.embeddings.EmbeddingEvaluator</code> for more details.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the results of the embedding evaluation, including   clustering metrics and optionally, correlation with external data.</p> Source code in <code>src\\ibbi\\evaluate\\__init__.py</code> <pre><code>def embeddings(\n    self,\n    dataset,\n    evaluation_level: str = \"image\",\n    use_umap: bool = True,\n    extract_kwargs: Optional[dict[str, Any]] = None,\n    batch_size: int = 32,\n    **kwargs,\n):\n    \"\"\"Evaluates the quality of the model's feature embeddings.\n\n    This method extracts feature embeddings from the provided dataset. It can operate\n    at two levels: 'image' (extracting one embedding per image) or 'object'\n    (extracting an embedding for each annotated object in each image). The quality of\n    these embeddings is then assessed using clustering algorithms and a suite of\n    internal and external validation metrics.\n\n    Args:\n        dataset (iterable): An iterable dataset where each item contains an 'image' key.\n                            For 'object' level evaluation, items should also contain 'objects'\n                            with 'bbox' and 'category' keys.\n        evaluation_level (str, optional): The level at which to evaluate embeddings.\n                                          Can be \"image\" or \"object\". Defaults to \"image\".\n        use_umap (bool, optional): If True, applies UMAP for dimensionality reduction\n                                   before clustering. Defaults to True.\n        extract_kwargs (Optional[dict[str, Any]], optional): Keyword arguments to be passed\n            to the model's `extract_features` method. Defaults to None.\n        batch_size (int, optional): The batch size for GPU distance matrix calculation.\n                                    Defaults to 32.\n        **kwargs: Additional keyword arguments to be passed to the `EmbeddingEvaluator`.\n                  See `ibbi.evaluate.embeddings.EmbeddingEvaluator` for more details.\n\n    Returns:\n        dict: A dictionary containing the results of the embedding evaluation, including\n              clustering metrics and optionally, correlation with external data.\n    \"\"\"\n    if extract_kwargs is None:\n        extract_kwargs = {}\n    if evaluation_level not in [\"image\", \"object\"]:\n        raise ValueError(\"evaluation_level must be either 'image' or 'object'.\")\n\n    print(f\"Extracting embeddings for evaluation at the '{evaluation_level}' level...\")\n    embeddings_list = []\n    true_labels = []\n    valid_indices = []\n\n    # Pre-calculate label mappings for efficiency\n    unique_labels_lst = list(set(cat for item in dataset for cat in item.get(\"objects\", {}).get(\"category\", [])))\n    unique_labels = sorted(unique_labels_lst)\n    name_to_idx = {name: i for i, name in enumerate(unique_labels)}\n    idx_to_name = dict(enumerate(unique_labels))\n\n    for i, item in enumerate(tqdm(dataset)):\n        if evaluation_level == \"image\":\n            embedding = self.model.extract_features(item[\"image\"], **extract_kwargs)\n            if embedding is not None:\n                embeddings_list.append(embedding)\n                if \"objects\" in item and \"category\" in item[\"objects\"] and item[\"objects\"][\"category\"]:\n                    label_name = item[\"objects\"][\"category\"][0]\n                    if label_name in name_to_idx:\n                        true_labels.append(name_to_idx[label_name])\n                        valid_indices.append(len(embeddings_list) - 1)\n\n        elif evaluation_level == \"object\":\n            if \"objects\" not in item or \"bbox\" not in item[\"objects\"] or \"category\" not in item[\"objects\"]:\n                continue\n\n            original_image = item[\"image\"]\n            for j, bbox in enumerate(item[\"objects\"][\"bbox\"]):\n                x, y, w, h = bbox\n                if w &gt; 0 and h &gt; 0:\n                    cropped_image = original_image.crop((x, y, x + w, y + h))\n                    embedding = self.model.extract_features(cropped_image, **extract_kwargs)\n                    if embedding is not None:\n                        embeddings_list.append(embedding)\n                        label_name = item[\"objects\"][\"category\"][j]\n                        if label_name in name_to_idx:\n                            true_labels.append(name_to_idx[label_name])\n                            valid_indices.append(len(embeddings_list) - 1)\n\n    if not embeddings_list:\n        print(\"Warning: Could not extract any valid embeddings from the dataset.\")\n        return {}\n\n    embeddings = np.array([emb.cpu().numpy().flatten() for emb in embeddings_list])\n    evaluator = EmbeddingEvaluator(embeddings, use_umap=use_umap, **kwargs)\n\n    results = {}\n    results[\"internal_cluster_validation\"] = evaluator.evaluate_cluster_structure()\n\n    if true_labels:\n        true_labels = np.array(true_labels)\n        results[\"external_cluster_validation\"] = evaluator.evaluate_against_truth(true_labels)\n        results[\"sample_results\"] = evaluator.get_sample_results(true_labels, label_map=idx_to_name)\n\n        try:\n            if len(np.unique(true_labels)) &gt;= 3:\n                valid_embeddings = embeddings[valid_indices]\n                evaluator_for_mantel = EmbeddingEvaluator(valid_embeddings, use_umap=False)\n                mantel_corr, p_val, n, per_class_df = evaluator_for_mantel.compare_to_distance_matrix(\n                    true_labels, label_map=idx_to_name, batch_size=batch_size\n                )\n                results[\"mantel_correlation\"] = {\"r\": mantel_corr, \"p_value\": p_val, \"n_items\": n}\n                results[\"per_class_centroids\"] = per_class_df\n            else:\n                print(\"Not enough unique labels in the dataset subset to run the Mantel test.\")\n        except (ImportError, FileNotFoundError, ValueError) as e:\n            print(f\"Could not run Mantel test: {e}\")\n    else:\n        print(\"Dataset does not have the required 'objects' and 'category' fields for external validation.\")\n        results[\"sample_results\"] = evaluator.get_sample_results()\n\n    return results\n</code></pre>"},{"location":"api/#ibbi.Evaluator.object_classification","title":"<code>object_classification(dataset, iou_thresholds=0.5, predict_kwargs=None, **kwargs)</code>","text":"<p>Runs a comprehensive object detection and classification performance analysis.</p> <p>This method assesses the model's ability to both accurately localize and correctly classify objects within a dataset. It iterates through the provided dataset, gathering ground truth information and generating model predictions. These are then passed to the <code>object_classification_performance</code> function to compute a detailed suite of metrics.</p> <p>The evaluation provides a holistic view of performance, combining traditional object detection metrics (like mAP) with a full suite of classification metrics for each IoU threshold.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>iterable</code> <p>An iterable dataset where each item is a dictionary-like object                 containing at least an 'image' key. For evaluation, items should                 also contain an 'objects' key, which is a dictionary with 'bbox'                 and 'category' keys.</p> required <code>iou_thresholds</code> <code>Union[float, list[float]]</code> <p>The IoU threshold(s) at which to compute mAP and classification metrics. Can be a single float or a list of floats. Defaults to 0.5.</p> <code>0.5</code> <code>predict_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>A dictionary of keyword arguments to be passed directly to the model's <code>predict</code> method during evaluation. This is useful for model-specific parameters like <code>text_prompt</code> for zero-shot models. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to the underlying       <code>object_classification_performance</code> function (e.g., <code>average</code>, <code>zero_division</code>).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing a comprehensive set of object detection and   classification metrics, including per-iou threshold classification performance,   and a detailed object-level performance table.</p> Source code in <code>src\\ibbi\\evaluate\\__init__.py</code> <pre><code>def object_classification(\n    self, dataset, iou_thresholds: Union[float, list[float]] = 0.5, predict_kwargs: Optional[dict[str, Any]] = None, **kwargs\n):\n    \"\"\"Runs a comprehensive object detection and classification performance analysis.\n\n    This method assesses the model's ability to both accurately localize and correctly\n    classify objects within a dataset. It iterates through the provided dataset, gathering\n    ground truth information and generating model predictions. These are then passed to the\n    `object_classification_performance` function to compute a detailed suite of metrics.\n\n    The evaluation provides a holistic view of performance, combining traditional object\n    detection metrics (like mAP) with a full suite of classification metrics for each IoU\n    threshold.\n\n    Args:\n        dataset (iterable): An iterable dataset where each item is a dictionary-like object\n                            containing at least an 'image' key. For evaluation, items should\n                            also contain an 'objects' key, which is a dictionary with 'bbox'\n                            and 'category' keys.\n        iou_thresholds (Union[float, list[float]], optional): The IoU threshold(s) at which\n            to compute mAP and classification metrics. Can be a single float or a list of floats.\n            Defaults to 0.5.\n        predict_kwargs (Optional[dict[str, Any]], optional): A dictionary of keyword arguments\n            to be passed directly to the model's `predict` method during evaluation.\n            This is useful for model-specific parameters like `text_prompt` for zero-shot models.\n            Defaults to None.\n        **kwargs: Additional keyword arguments to be passed to the underlying\n                  `object_classification_performance` function (e.g., `average`, `zero_division`).\n\n    Returns:\n        dict: A dictionary containing a comprehensive set of object detection and\n              classification metrics, including per-iou threshold classification performance,\n              and a detailed object-level performance table.\n    \"\"\"\n    if predict_kwargs is None:\n        predict_kwargs = {}\n\n    # Set classes for GroundingDINO before evaluation\n    if isinstance(self.model, GroundingDINOModel):\n        if \"text_prompt\" in predict_kwargs:\n            self.model.set_classes(predict_kwargs[\"text_prompt\"])\n\n    print(\"Running object classification evaluation...\")\n\n    if isinstance(self.model, (HuggingFaceFeatureExtractor, UntrainedFeatureExtractor)):\n        print(\"Warning: Object classification evaluation is not supported for pure feature extractors.\")\n        return {}\n\n    if isinstance(self.model, (GroundingDINOModel, YOLOWorldModel)):\n        if \"text_prompt\" not in predict_kwargs and not self.model.get_classes():\n            print(\"Warning: Zero-shot model has no classes set. Please provide a 'text_prompt' in 'predict_kwargs'.\")\n            return {}\n\n    gt_boxes, gt_labels, gt_image_ids, gt_label_names = [], [], [], []\n    pred_results_with_probs = []  # Full prediction result per image\n    # Initialize model_classes before the loop.\n    model_classes: list[str] = []\n    if isinstance(self.model, (GroundingDINOModel)):\n        if hasattr(self.model, \"get_classes\") and callable(self.model.get_classes):\n            raw_model_classes = self.model.get_classes()\n            if isinstance(raw_model_classes, dict):\n                model_classes = list(raw_model_classes.values())\n            else:\n                model_classes = raw_model_classes\n    class_name_to_idx: dict[str, int] = {}\n    idx_to_name: dict[int, str] = {}\n\n    print(\"Extracting ground truth and making predictions...\")\n    predict_kwargs_for_call = {**predict_kwargs, \"include_full_probabilities\": True}\n\n    for i, item in enumerate(tqdm(dataset)):\n        # Make the first prediction to set classes for YOLOWorld\n        results = self.model.predict(item[\"image\"], verbose=False, **predict_kwargs_for_call)\n        pred_results_with_probs.append(results)\n\n        if not model_classes:\n            if not hasattr(self.model, \"get_classes\") or not callable(self.model.get_classes):\n                print(\"Warning: Model does not have a 'get_classes' method for class mapping. Skipping evaluation.\")\n                return {}\n\n            raw_model_classes = self.model.get_classes()\n            if isinstance(raw_model_classes, dict):\n                model_classes: list[str] = list(raw_model_classes.values())\n            else:\n                model_classes: list[str] = raw_model_classes\n\n            if not model_classes:\n                print(\"Warning: Model returned an empty class list. Cannot proceed with classification-dependent metrics.\")\n                return {}\n\n            class_name_to_idx = {v: k for k, v in enumerate(model_classes)}\n            idx_to_name = dict(enumerate(model_classes))\n\n        # --- Extract Ground Truth ---\n        if \"objects\" in item and \"bbox\" in item[\"objects\"] and \"category\" in item[\"objects\"]:\n            for j in range(len(item[\"objects\"][\"category\"])):\n                label_name = item[\"objects\"][\"category\"][j]\n                gt_label_names.append(label_name)\n                bbox = item[\"objects\"][\"bbox\"][j]\n                x1, y1, w, h = bbox\n                x2 = x1 + w\n                y2 = y1 + h\n                gt_boxes.append([x1, y1, x2, y2])\n                gt_labels.append(class_name_to_idx.get(label_name, -1))\n\n                gt_image_ids.append(i)\n\n    # The GT and raw prediction data is prepared. Now run the core evaluation logic.\n    performance_results = object_classification_performance(\n        np.array(gt_boxes),\n        gt_labels,\n        gt_image_ids,\n        pred_results_with_probs,\n        gt_label_names=gt_label_names,\n        iou_thresholds=iou_thresholds,\n        model_classes=model_classes,\n        idx_to_name=idx_to_name,\n        **kwargs,\n    )\n\n    # Apply naming to the mAP results\n    if \"per_class_AP_at_last_iou\" in performance_results:\n        class_aps = performance_results[\"per_class_AP_at_last_iou\"]\n        named_class_aps = {idx_to_name.get(class_id, class_id): ap for class_id, ap in class_aps.items()}\n        performance_results[\"per_class_AP_at_last_iou\"] = named_class_aps\n\n    return performance_results\n</code></pre>"},{"location":"api/#ibbi.Explainer","title":"<code>Explainer</code>","text":"<p>A wrapper for LIME and SHAP explainability methods.</p> <p>This class provides a simple interface to generate model explanations using either LIME (Local Interpretable Model-agnostic Explanations) or SHAP (SHapley Additive exPlanations). It is designed to work with any model created using <code>ibbi.create_model</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelType</code> <p>An instantiated model from <code>ibbi.create_model</code>.</p> required Source code in <code>src\\ibbi\\explain\\__init__.py</code> <pre><code>class Explainer:\n    \"\"\"A wrapper for LIME and SHAP explainability methods.\n\n    This class provides a simple interface to generate model explanations using\n    either LIME (Local Interpretable Model-agnostic Explanations) or SHAP\n    (SHapley Additive exPlanations). It is designed to work with any model\n    created using `ibbi.create_model`.\n\n    Args:\n        model (ModelType): An instantiated model from `ibbi.create_model`.\n    \"\"\"\n\n    def __init__(self, model: ModelType):\n        \"\"\"A wrapper for LIME and SHAP explainability methods.\n\n        This class provides a simple interface to generate model explanations using\n        either LIME (Local Interpretable Model-agnostic Explanations) or SHAP\n        (SHapley Additive exPlanations). It is designed to work with any model\n        created using `ibbi.create_model`.\n\n        Args:\n            model (ModelType): An instantiated model from `ibbi.create_model`.\n        \"\"\"\n        self.model = model\n\n    def with_lime(self, image, **kwargs):\n        \"\"\"Generates a LIME explanation for a single image.\n\n        LIME provides a local, intuitive explanation by showing which parts of an image\n        contributed most to a specific prediction. This method is a wrapper around\n        the `explain_with_lime` function.\n\n        Args:\n            image (PIL.Image.Image): The single image to be explained.\n            **kwargs: Additional keyword arguments to be passed to the underlying\n                      `ibbi.explain.lime.explain_with_lime` function. Common arguments\n                      include `image_size`, `batch_size`, `num_samples`, `top_labels`,\n                      and `num_features`.\n\n        Returns:\n            tuple[lime_image.ImageExplanation, PIL.Image.Image]: A tuple containing the LIME\n            explanation object and the original image. The explanation object can be\n            visualized using `ibbi.plot_lime_explanation`.\n        \"\"\"\n        return explain_with_lime(self.model, image, **kwargs)\n\n    def with_shap(self, explain_dataset, background_dataset, **kwargs):\n        \"\"\"Generates SHAP explanations for a set of images.\n\n        SHAP (SHapley Additive exPlanations) provides robust, theoretically-grounded\n        explanations by attributing a model's prediction to its input features. This\n        method is a wrapper around the `explain_with_shap` function and requires a\n        background dataset to integrate out features.\n\n        Args:\n            explain_dataset (list): A list of dictionaries, where each dictionary\n                                    represents an image to be explained (e.g., `[{'image': img1}, {'image': img2}]`).\n            background_dataset (list): A list of dictionaries representing a background dataset,\n                                       used by SHAP to simulate feature absence.\n            **kwargs: Additional keyword arguments to be passed to the underlying\n                      `ibbi.explain.shap.explain_with_shap` function. Common arguments\n                      include `num_explain_samples`, `max_evals`, `image_size`, and `text_prompt`.\n\n        Returns:\n            shap.Explanation: A SHAP Explanation object containing the SHAP values for each\n                              image and each class. This object can be visualized using\n                              `ibbi.plot_shap_explanation`.\n        \"\"\"\n        return explain_with_shap(self.model, explain_dataset, background_dataset, **kwargs)\n</code></pre>"},{"location":"api/#ibbi.Explainer.__init__","title":"<code>__init__(model)</code>","text":"<p>A wrapper for LIME and SHAP explainability methods.</p> <p>This class provides a simple interface to generate model explanations using either LIME (Local Interpretable Model-agnostic Explanations) or SHAP (SHapley Additive exPlanations). It is designed to work with any model created using <code>ibbi.create_model</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelType</code> <p>An instantiated model from <code>ibbi.create_model</code>.</p> required Source code in <code>src\\ibbi\\explain\\__init__.py</code> <pre><code>def __init__(self, model: ModelType):\n    \"\"\"A wrapper for LIME and SHAP explainability methods.\n\n    This class provides a simple interface to generate model explanations using\n    either LIME (Local Interpretable Model-agnostic Explanations) or SHAP\n    (SHapley Additive exPlanations). It is designed to work with any model\n    created using `ibbi.create_model`.\n\n    Args:\n        model (ModelType): An instantiated model from `ibbi.create_model`.\n    \"\"\"\n    self.model = model\n</code></pre>"},{"location":"api/#ibbi.Explainer.with_lime","title":"<code>with_lime(image, **kwargs)</code>","text":"<p>Generates a LIME explanation for a single image.</p> <p>LIME provides a local, intuitive explanation by showing which parts of an image contributed most to a specific prediction. This method is a wrapper around the <code>explain_with_lime</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The single image to be explained.</p> required <code>**kwargs</code> <p>Additional keyword arguments to be passed to the underlying       <code>ibbi.explain.lime.explain_with_lime</code> function. Common arguments       include <code>image_size</code>, <code>batch_size</code>, <code>num_samples</code>, <code>top_labels</code>,       and <code>num_features</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <p>tuple[lime_image.ImageExplanation, PIL.Image.Image]: A tuple containing the LIME</p> <p>explanation object and the original image. The explanation object can be</p> <p>visualized using <code>ibbi.plot_lime_explanation</code>.</p> Source code in <code>src\\ibbi\\explain\\__init__.py</code> <pre><code>def with_lime(self, image, **kwargs):\n    \"\"\"Generates a LIME explanation for a single image.\n\n    LIME provides a local, intuitive explanation by showing which parts of an image\n    contributed most to a specific prediction. This method is a wrapper around\n    the `explain_with_lime` function.\n\n    Args:\n        image (PIL.Image.Image): The single image to be explained.\n        **kwargs: Additional keyword arguments to be passed to the underlying\n                  `ibbi.explain.lime.explain_with_lime` function. Common arguments\n                  include `image_size`, `batch_size`, `num_samples`, `top_labels`,\n                  and `num_features`.\n\n    Returns:\n        tuple[lime_image.ImageExplanation, PIL.Image.Image]: A tuple containing the LIME\n        explanation object and the original image. The explanation object can be\n        visualized using `ibbi.plot_lime_explanation`.\n    \"\"\"\n    return explain_with_lime(self.model, image, **kwargs)\n</code></pre>"},{"location":"api/#ibbi.Explainer.with_shap","title":"<code>with_shap(explain_dataset, background_dataset, **kwargs)</code>","text":"<p>Generates SHAP explanations for a set of images.</p> <p>SHAP (SHapley Additive exPlanations) provides robust, theoretically-grounded explanations by attributing a model's prediction to its input features. This method is a wrapper around the <code>explain_with_shap</code> function and requires a background dataset to integrate out features.</p> <p>Parameters:</p> Name Type Description Default <code>explain_dataset</code> <code>list</code> <p>A list of dictionaries, where each dictionary                     represents an image to be explained (e.g., <code>[{'image': img1}, {'image': img2}]</code>).</p> required <code>background_dataset</code> <code>list</code> <p>A list of dictionaries representing a background dataset,                        used by SHAP to simulate feature absence.</p> required <code>**kwargs</code> <p>Additional keyword arguments to be passed to the underlying       <code>ibbi.explain.shap.explain_with_shap</code> function. Common arguments       include <code>num_explain_samples</code>, <code>max_evals</code>, <code>image_size</code>, and <code>text_prompt</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <p>shap.Explanation: A SHAP Explanation object containing the SHAP values for each               image and each class. This object can be visualized using               <code>ibbi.plot_shap_explanation</code>.</p> Source code in <code>src\\ibbi\\explain\\__init__.py</code> <pre><code>def with_shap(self, explain_dataset, background_dataset, **kwargs):\n    \"\"\"Generates SHAP explanations for a set of images.\n\n    SHAP (SHapley Additive exPlanations) provides robust, theoretically-grounded\n    explanations by attributing a model's prediction to its input features. This\n    method is a wrapper around the `explain_with_shap` function and requires a\n    background dataset to integrate out features.\n\n    Args:\n        explain_dataset (list): A list of dictionaries, where each dictionary\n                                represents an image to be explained (e.g., `[{'image': img1}, {'image': img2}]`).\n        background_dataset (list): A list of dictionaries representing a background dataset,\n                                   used by SHAP to simulate feature absence.\n        **kwargs: Additional keyword arguments to be passed to the underlying\n                  `ibbi.explain.shap.explain_with_shap` function. Common arguments\n                  include `num_explain_samples`, `max_evals`, `image_size`, and `text_prompt`.\n\n    Returns:\n        shap.Explanation: A SHAP Explanation object containing the SHAP values for each\n                          image and each class. This object can be visualized using\n                          `ibbi.plot_shap_explanation`.\n    \"\"\"\n    return explain_with_shap(self.model, explain_dataset, background_dataset, **kwargs)\n</code></pre>"},{"location":"api/#ibbi.clean_cache","title":"<code>clean_cache()</code>","text":"<p>Removes the entire ibbi cache directory.</p> <p>This function will permanently delete all downloaded models and datasets associated with the <code>ibbi</code> package's cache. This can be useful for forcing a fresh download of all assets or for freeing up disk space.</p> Source code in <code>src\\ibbi\\utils\\cache.py</code> <pre><code>def clean_cache():\n    \"\"\"Removes the entire ibbi cache directory.\n\n    This function will permanently delete all downloaded models and datasets\n    associated with the `ibbi` package's cache. This can be useful for forcing\n    a fresh download of all assets or for freeing up disk space.\n    \"\"\"\n    cache_dir = get_cache_dir()\n    if cache_dir.exists():\n        print(f\"Removing cache directory: {cache_dir}\")\n        shutil.rmtree(cache_dir)\n        print(\"Cache cleaned successfully.\")\n    else:\n        print(\"Cache directory not found. Nothing to clean.\")\n</code></pre>"},{"location":"api/#ibbi.create_model","title":"<code>create_model(model_name, pretrained=False, **kwargs)</code>","text":"<p>Creates a model from a name or a task-based alias.</p> <p>This function is the main entry point for instantiating models within the <code>ibbi</code> package. It uses a model registry to look up and create a model instance based on the provided <code>model_name</code>. Users can either specify the exact name of a model or use a convenient, task-based alias (e.g., \"species_classifier\").</p> <p>When <code>pretrained=True</code>, the function will download the model's weights from the Hugging Face Hub and cache them locally for future use.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name or alias of the model to create. A list of available               model names and aliases can be obtained using <code>ibbi.list_models()</code>.</p> required <code>pretrained</code> <code>bool</code> <p>If True, loads pretrained weights for the model.                          Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments that will be passed to the underlying             model's factory function. This allows for advanced customization.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ModelType</code> <code>ModelType</code> <p>An instantiated model object ready for prediction or feature extraction.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the provided <code>model_name</code> or its resolved alias is not found in the       model registry.</p> Source code in <code>src\\ibbi\\__init__.py</code> <pre><code>def create_model(model_name: str, pretrained: bool = False, **kwargs: Any) -&gt; ModelType:\n    \"\"\"Creates a model from a name or a task-based alias.\n\n    This function is the main entry point for instantiating models within the `ibbi`\n    package. It uses a model registry to look up and create a model instance based on\n    the provided `model_name`. Users can either specify the exact name of a model\n    or use a convenient, task-based alias (e.g., \"species_classifier\").\n\n    When `pretrained=True`, the function will download the model's weights from the\n    Hugging Face Hub and cache them locally for future use.\n\n    Args:\n        model_name (str): The name or alias of the model to create. A list of available\n                          model names and aliases can be obtained using `ibbi.list_models()`.\n        pretrained (bool, optional): If True, loads pretrained weights for the model.\n                                     Defaults to False.\n        **kwargs (Any): Additional keyword arguments that will be passed to the underlying\n                        model's factory function. This allows for advanced customization.\n\n    Returns:\n        ModelType: An instantiated model object ready for prediction or feature extraction.\n\n    Raises:\n        KeyError: If the provided `model_name` or its resolved alias is not found in the\n                  model registry.\n    \"\"\"\n    # Resolve alias if used\n    if model_name in MODEL_ALIASES:\n        model_name = MODEL_ALIASES[model_name]\n\n    if model_name not in model_registry:\n        available = \", \".join(model_registry.keys())\n        aliases = \", \".join(MODEL_ALIASES.keys())\n        raise KeyError(f\"Model '{model_name}' not found. Available models: [{available}]. Available aliases: [{aliases}].\")\n\n    model_factory = model_registry[model_name]\n    model = model_factory(pretrained=pretrained, **kwargs)\n    return model\n</code></pre>"},{"location":"api/#ibbi.get_cache_dir","title":"<code>get_cache_dir()</code>","text":"<p>Gets the cache directory for the ibbi package.</p> <p>This function determines the appropriate directory for storing cached files, such as downloaded model weights and datasets. It first checks for a custom path set by the <code>IBBI_CACHE_DIR</code> environment variable. If the variable is not set, it defaults to a standard user cache location (<code>~/.cache/ibbi</code>).</p> <p>The function also ensures that the cache directory exists by creating it if it does not already.</p> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>A <code>pathlib.Path</code> object representing the path to the cache directory.</p> Source code in <code>src\\ibbi\\utils\\cache.py</code> <pre><code>def get_cache_dir() -&gt; Path:\n    \"\"\"Gets the cache directory for the ibbi package.\n\n    This function determines the appropriate directory for storing cached files,\n    such as downloaded model weights and datasets. It first checks for a custom path\n    set by the `IBBI_CACHE_DIR` environment variable. If the variable is not set,\n    it defaults to a standard user cache location (`~/.cache/ibbi`).\n\n    The function also ensures that the cache directory exists by creating it if it\n    does not already.\n\n    Returns:\n        Path: A `pathlib.Path` object representing the path to the cache directory.\n    \"\"\"\n    # Check for the custom environment variable\n    cache_env_var = os.getenv(\"IBBI_CACHE_DIR\")\n    if cache_env_var:\n        cache_dir = Path(cache_env_var)\n    else:\n        # Default to a user's home cache directory\n        cache_dir = Path.home() / \".cache\" / \"ibbi\"\n\n    # Create the directory if it doesn't exist\n    cache_dir.mkdir(parents=True, exist_ok=True)\n    return cache_dir\n</code></pre>"},{"location":"api/#ibbi.get_dataset","title":"<code>get_dataset(repo_id='IBBI-bio/ibbi_test_data', local_dir='ibbi_test_data', split='train', **kwargs)</code>","text":"<p>Downloads and loads a dataset from the Hugging Face Hub.</p> <p>This function facilitates the use of datasets hosted on the Hugging Face Hub by handling the download and caching process. It downloads the dataset to a local directory, and on subsequent calls, it will load the data directly from the local cache to save time and bandwidth.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The repository ID of the dataset on the Hugging Face Hub.                      Defaults to \"IBBI-bio/ibbi_test_data\".</p> <code>'IBBI-bio/ibbi_test_data'</code> <code>local_dir</code> <code>str</code> <p>The name of the local directory where the dataset will be stored.                        Defaults to \"ibbi_test_data\".</p> <code>'ibbi_test_data'</code> <code>split</code> <code>str</code> <p>The name of the dataset split to load (e.g., \"train\", \"test\", \"validation\").                    Defaults to \"train\".</p> <code>'train'</code> <code>**kwargs</code> <p>Additional keyword arguments that will be passed directly to the       <code>datasets.load_dataset</code> function. This allows for advanced customization       of the data loading process.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>The loaded dataset as a <code>datasets.Dataset</code> object.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the object loaded for the specified split is not of type <code>datasets.Dataset</code>.</p> Source code in <code>src\\ibbi\\utils\\data.py</code> <pre><code>def get_dataset(\n    repo_id: str = \"IBBI-bio/ibbi_test_data\",\n    local_dir: str = \"ibbi_test_data\",\n    split: str = \"train\",\n    **kwargs,\n) -&gt; Dataset:\n    \"\"\"Downloads and loads a dataset from the Hugging Face Hub.\n\n    This function facilitates the use of datasets hosted on the Hugging Face Hub by\n    handling the download and caching process. It downloads the dataset to a local\n    directory, and on subsequent calls, it will load the data directly from the local\n    cache to save time and bandwidth.\n\n    Args:\n        repo_id (str, optional): The repository ID of the dataset on the Hugging Face Hub.\n                                 Defaults to \"IBBI-bio/ibbi_test_data\".\n        local_dir (str, optional): The name of the local directory where the dataset will be stored.\n                                   Defaults to \"ibbi_test_data\".\n        split (str, optional): The name of the dataset split to load (e.g., \"train\", \"test\", \"validation\").\n                               Defaults to \"train\".\n        **kwargs: Additional keyword arguments that will be passed directly to the\n                  `datasets.load_dataset` function. This allows for advanced customization\n                  of the data loading process.\n\n    Returns:\n        Dataset: The loaded dataset as a `datasets.Dataset` object.\n\n    Raises:\n        TypeError: If the object loaded for the specified split is not of type `datasets.Dataset`.\n    \"\"\"\n    dataset_path = Path(local_dir)\n\n    if not dataset_path.exists():\n        print(f\"Dataset not found locally. Downloading from '{repo_id}' to '{dataset_path}'...\")\n        snapshot_download(repo_id=repo_id, repo_type=\"dataset\", local_dir=str(dataset_path))\n        print(\"Download complete.\")\n    else:\n        print(f\"Found cached dataset at '{dataset_path}'. Loading from disk.\")\n\n    try:\n        dataset: Union[Dataset, DatasetDict, IterableDataset, IterableDatasetDict] = load_dataset(\n            str(dataset_path), split=split, trust_remote_code=True, **kwargs\n        )\n\n        if not isinstance(dataset, Dataset):\n            raise TypeError(f\"Expected a 'Dataset' object for split '{split}', but received type '{type(dataset).__name__}'.\")\n\n        print(\"Dataset loaded successfully.\")\n        return dataset\n    except Exception as e:\n        print(f\"Failed to load dataset from '{dataset_path}'. Please check the path and your connection.\")\n        raise e\n</code></pre>"},{"location":"api/#ibbi.get_ood_dataset","title":"<code>get_ood_dataset(repo_id='IBBI-bio/ibbi_ood_data', local_dir='ibbi_ood_data', split='train', **kwargs)</code>","text":"<p>Downloads and loads the out-of-distribution (OOD) dataset from the Hugging Face Hub.</p> <p>This function handles the download and caching of the OOD dataset. On subsequent calls, it will load the data directly from the local cache.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The repository ID of the OOD dataset on the Hugging Face Hub.                      Defaults to \"IBBI-bio/ibbi_ood_data\".</p> <code>'IBBI-bio/ibbi_ood_data'</code> <code>local_dir</code> <code>str</code> <p>The name of the local directory where the dataset will be stored.                        Defaults to \"ibbi_ood_data\".</p> <code>'ibbi_ood_data'</code> <code>split</code> <code>str</code> <p>The name of the dataset split to load. Defaults to \"train\".</p> <code>'train'</code> <code>**kwargs</code> <p>Additional keyword arguments for the <code>datasets.load_dataset</code> function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>The loaded OOD dataset as a <code>datasets.Dataset</code> object.</p> Source code in <code>src\\ibbi\\utils\\data.py</code> <pre><code>def get_ood_dataset(\n    repo_id: str = \"IBBI-bio/ibbi_ood_data\",\n    local_dir: str = \"ibbi_ood_data\",\n    split: str = \"train\",\n    **kwargs,\n) -&gt; Dataset:\n    \"\"\"Downloads and loads the out-of-distribution (OOD) dataset from the Hugging Face Hub.\n\n    This function handles the download and caching of the OOD dataset. On subsequent\n    calls, it will load the data directly from the local cache.\n\n    Args:\n        repo_id (str, optional): The repository ID of the OOD dataset on the Hugging Face Hub.\n                                 Defaults to \"IBBI-bio/ibbi_ood_data\".\n        local_dir (str, optional): The name of the local directory where the dataset will be stored.\n                                   Defaults to \"ibbi_ood_data\".\n        split (str, optional): The name of the dataset split to load. Defaults to \"train\".\n        **kwargs: Additional keyword arguments for the `datasets.load_dataset` function.\n\n    Returns:\n        Dataset: The loaded OOD dataset as a `datasets.Dataset` object.\n    \"\"\"\n    return get_dataset(repo_id=repo_id, local_dir=local_dir, split=split, **kwargs)\n</code></pre>"},{"location":"api/#ibbi.get_shap_background_dataset","title":"<code>get_shap_background_dataset(image_size=(224, 224))</code>","text":"<p>Downloads, unzips, and loads the default IBBI SHAP background dataset.</p> <p>This function is specifically designed to fetch the background dataset required for the SHAP (SHapley Additive exPlanations) explainability method. It handles the download of a zip archive from the Hugging Face Hub, extracts its contents, and loads the images into memory. The data is stored in the package's central cache directory to avoid re-downloads.</p> <p>Parameters:</p> Name Type Description Default <code>image_size</code> <code>tuple[int, int]</code> <p>The target size (width, height) to which the                                     background images will be resized. This should                                     match the input size expected by the model being                                     explained. Defaults to (224, 224).</p> <code>(224, 224)</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of dictionaries, where each dictionary has an \"image\" key with a         resized PIL Image object. This format is ready to be used with the         <code>ibbi.Explainer.with_shap</code> method.</p> Source code in <code>src\\ibbi\\utils\\data.py</code> <pre><code>def get_shap_background_dataset(image_size: tuple[int, int] = (224, 224)) -&gt; list[dict]:\n    \"\"\"Downloads, unzips, and loads the default IBBI SHAP background dataset.\n\n    This function is specifically designed to fetch the background dataset required for the\n    SHAP (SHapley Additive exPlanations) explainability method. It handles the download of a\n    zip archive from the Hugging Face Hub, extracts its contents, and loads the images into\n    memory. The data is stored in the package's central cache directory to avoid re-downloads.\n\n    Args:\n        image_size (tuple[int, int], optional): The target size (width, height) to which the\n                                                background images will be resized. This should\n                                                match the input size expected by the model being\n                                                explained. Defaults to (224, 224).\n\n    Returns:\n        list[dict]: A list of dictionaries, where each dictionary has an \"image\" key with a\n                    resized PIL Image object. This format is ready to be used with the\n                    `ibbi.Explainer.with_shap` method.\n    \"\"\"\n    repo_id = \"IBBI-bio/ibbi_shap_dataset\"\n    filename = \"ibbi_shap_dataset.zip\"\n    cache_dir = get_cache_dir()\n    unzip_dir = cache_dir / \"unzipped_shap_data\"\n    image_dir = unzip_dir / \"shap_dataset\" / \"images\" / \"train\"\n\n    if not image_dir.exists() or not any(image_dir.iterdir()):\n        print(f\"SHAP background data not found in cache. Downloading from '{repo_id}'...\")\n        downloaded_zip_path = hf_hub_download(repo_id=repo_id, filename=filename, repo_type=\"dataset\", cache_dir=str(cache_dir))\n\n        print(\"Decompressing SHAP background dataset...\")\n        unzip_dir.mkdir(exist_ok=True)\n        with zipfile.ZipFile(downloaded_zip_path, \"r\") as zip_ref:\n            zip_ref.extractall(unzip_dir)\n    else:\n        print(\"Found cached SHAP background data. Loading from disk.\")\n\n    background_images = []\n    print(f\"Loading and resizing SHAP background images to {image_size}...\")\n    image_paths = list(image_dir.glob(\"*\"))\n\n    for img_path in image_paths:\n        with Image.open(img_path) as img:\n            resized_img = img.resize(image_size)\n            background_images.append({\"image\": resized_img.copy()})\n\n    print(\"SHAP background dataset loaded and resized successfully.\")\n    return background_images\n</code></pre>"},{"location":"api/#ibbi.list_models","title":"<code>list_models(as_df=False)</code>","text":"<p>Displays or returns a summary of available models and their key information.</p> <p>This function reads the model summary CSV file included with the package, which contains a comprehensive list of all available models, their tasks, and key performance metrics. It can either print this information to the console in a human-readable format or return it as a pandas DataFrame for programmatic access.</p> <p>Parameters:</p> Name Type Description Default <code>as_df</code> <code>bool</code> <p>If True, the function returns the model information as a                     pandas DataFrame. If False (the default), it prints the                     information directly to the console.</p> <code>False</code> <p>Returns:</p> Type Description <p>pd.DataFrame or None: If <code>as_df</code> is True, a pandas DataFrame containing the model                   summary is returned. Otherwise, the function returns None.</p> Source code in <code>src\\ibbi\\utils\\info.py</code> <pre><code>def list_models(as_df: bool = False):\n    \"\"\"Displays or returns a summary of available models and their key information.\n\n    This function reads the model summary CSV file included with the package, which\n    contains a comprehensive list of all available models, their tasks, and key\n    performance metrics. It can either print this information to the console in a\n    human-readable format or return it as a pandas DataFrame for programmatic access.\n\n    Args:\n        as_df (bool, optional): If True, the function returns the model information as a\n                                pandas DataFrame. If False (the default), it prints the\n                                information directly to the console.\n\n    Returns:\n        pd.DataFrame or None: If `as_df` is True, a pandas DataFrame containing the model\n                              summary is returned. Otherwise, the function returns None.\n    \"\"\"\n    try:\n        # Find the path to the data file within the package\n        with resources.files(\"ibbi.data\").joinpath(\"ibbi_model_summary.csv\").open(\"r\") as f:\n            df = pd.read_csv(f)\n\n        if as_df:\n            return df\n        else:\n            print(\"Available IBBI Models:\")\n            print(df.to_string())\n\n    except FileNotFoundError:\n        print(\"Error: Model summary file not found.\")\n        return None\n</code></pre>"},{"location":"api/#ibbi.plot_lime_explanation","title":"<code>plot_lime_explanation(explanation, image, top_k=1, alpha=0.6)</code>","text":"<p>Plots a detailed LIME explanation with a red-to-green overlay.</p> <p>This function visualizes the output of <code>explain_with_lime</code>. It overlays the original image with a heatmap where green areas indicate features that positively contributed to the prediction, and red areas indicate negative contributions.</p> <p>Parameters:</p> Name Type Description Default <code>explanation</code> <code>ImageExplanation</code> <p>The explanation object generated by <code>explain_with_lime</code>.</p> required <code>image</code> <code>Image</code> <p>The original image that was explained.</p> required <code>top_k</code> <code>int</code> <p>The number of top classes to display explanations for. Defaults to 1.</p> <code>1</code> <code>alpha</code> <code>float</code> <p>The transparency of the color overlay. Defaults to 0.6.</p> <code>0.6</code> Source code in <code>src\\ibbi\\explain\\lime.py</code> <pre><code>def plot_lime_explanation(explanation: lime_image.ImageExplanation, image: Image.Image, top_k: int = 1, alpha: float = 0.6) -&gt; None:\n    \"\"\"Plots a detailed LIME explanation with a red-to-green overlay.\n\n    This function visualizes the output of `explain_with_lime`. It overlays the original\n    image with a heatmap where green areas indicate features that positively contributed\n    to the prediction, and red areas indicate negative contributions.\n\n    Args:\n        explanation (lime_image.ImageExplanation): The explanation object generated by `explain_with_lime`.\n        image (Image.Image): The original image that was explained.\n        top_k (int, optional): The number of top classes to display explanations for. Defaults to 1.\n        alpha (float, optional): The transparency of the color overlay. Defaults to 0.6.\n    \"\"\"\n    plt.figure(figsize=(5, 5))\n    plt.imshow(image)\n    plt.title(\"Original Image\")\n    plt.axis(\"off\")\n    plt.show()\n\n    segments = explanation.segments\n\n    for label in explanation.top_labels[:top_k]:  # type: ignore[attr-defined]\n        print(f\"\\n--- Explanation for Class Index: {label} ---\")\n\n        exp_for_label = explanation.local_exp.get(label)\n        if not exp_for_label:\n            print(f\"No explanation available for class {label}.\")\n            continue\n\n        weight_map = np.zeros(segments.shape, dtype=np.float32)\n        for feature, weight in exp_for_label:\n            weight_map[segments == feature] = weight\n\n        max_abs_weight = np.max(np.abs(weight_map))\n        if max_abs_weight == 0:\n            print(f\"No significant features found for class {label}.\")\n            fig, ax = plt.subplots(figsize=(6, 6))\n            ax.imshow(image)\n            ax.set_title(f\"LIME: No features for class {label}\")\n            ax.axis(\"off\")\n            plt.show()\n            continue\n\n        norm = mcolors.Normalize(vmin=-max_abs_weight, vmax=max_abs_weight)\n        cmap = plt.cm.RdYlGn  # type: ignore[attr-defined]\n\n        colored_overlay_rgba = cmap(norm(weight_map))\n        original_size = image.size\n        colored_overlay_resized = resize(\n            colored_overlay_rgba,\n            (original_size[1], original_size[0]),\n            anti_aliasing=True,\n            mode=\"constant\",\n        )\n\n        fig, ax = plt.subplots(figsize=(7, 6))\n        ax.imshow(image)\n        ax.imshow(colored_overlay_resized, alpha=alpha)  # type: ignore[arg-type]\n\n        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n        sm.set_array([])\n\n        cbar = fig.colorbar(sm, ax=ax, fraction=0.046, pad=0.04)\n        cbar.set_label(\"Feature Weight (Green: Positive, Red: Negative)\", rotation=270, labelpad=20)\n\n        ax.set_title(f\"LIME Explanation for Class Index: {label}\")\n        ax.axis(\"off\")\n        plt.show()\n</code></pre>"},{"location":"api/#ibbi.plot_shap_explanation","title":"<code>plot_shap_explanation(shap_explanation_for_single_image, model, top_k=5, text_prompt=None)</code>","text":"<p>Plots SHAP explanations for a SINGLE image.</p> <p>This function is designed to visualize the output of <code>explain_with_shap</code> for one image. It uses SHAP's built-in image plotting capabilities to show which parts of the image contributed to the model's predictions for the top-k classes.</p> <p>Parameters:</p> Name Type Description Default <code>shap_explanation_for_single_image</code> <code>Explanation</code> <p>A SHAP Explanation object for a single image.</p> required <code>model</code> <code>ModelType</code> <p>The <code>ibbi</code> model that was explained.</p> required <code>top_k</code> <code>int</code> <p>The number of top class explanations to plot. Defaults to 5.</p> <code>5</code> <code>text_prompt</code> <code>Optional[str]</code> <p>The text prompt used for explaining a zero-shot model. Defaults to None.</p> <code>None</code> Source code in <code>src\\ibbi\\explain\\shap.py</code> <pre><code>def plot_shap_explanation(\n    shap_explanation_for_single_image: shap.Explanation,\n    model: ModelType,\n    top_k: int = 5,\n    text_prompt: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Plots SHAP explanations for a SINGLE image.\n\n    This function is designed to visualize the output of `explain_with_shap` for one image.\n    It uses SHAP's built-in image plotting capabilities to show which parts of the image\n    contributed to the model's predictions for the top-k classes.\n\n    Args:\n        shap_explanation_for_single_image (shap.Explanation): A SHAP Explanation object for a single image.\n        model (ModelType): The `ibbi` model that was explained.\n        top_k (int, optional): The number of top class explanations to plot. Defaults to 5.\n        text_prompt (Optional[str], optional): The text prompt used for explaining a zero-shot model. Defaults to None.\n    \"\"\"\n    print(\"\\n--- Generating Explanations for Image ---\")\n\n    image_for_plotting = shap_explanation_for_single_image.data\n    shap_values = shap_explanation_for_single_image.values\n    class_names = np.array(shap_explanation_for_single_image.output_names)\n\n    prediction_fn = _prediction_wrapper(model, text_prompt=text_prompt)\n    image_norm = _prepare_image_for_shap(np.array(image_for_plotting))\n\n    prediction_scores = prediction_fn(image_norm[np.newaxis, ...])[0]\n\n    if len(prediction_scores) &gt; 1:\n        top_indices = np.argsort(prediction_scores)[-top_k:][::-1]\n    else:\n        top_indices = [0]\n\n    plt.figure(figsize=(5, 5))\n    plt.imshow(image_for_plotting)\n    plt.title(\"Original Image\")\n    plt.axis(\"off\")\n    plt.show()\n    shap_values_for_plot = shap_values[..., top_indices]  # type: ignore\n    class_names_for_plot = class_names[top_indices]\n\n    if np.all(shap_values == 0):\n        print(\"\u26a0\ufe0f  Warning: SHAP values are all zero. The plot will be empty.\")\n        print(\"   This can happen if the model's prediction is not sensitive to the masking.\")\n\n    shap.image_plot(\n        shap_values=[shap_values_for_plot] if isinstance(shap_values_for_plot, np.ndarray) else shap_values_for_plot,\n        pixel_values=image_for_plotting,\n        labels=np.array([class_names_for_plot]),\n        show=True,\n    )\n</code></pre>"},{"location":"usage/","title":"Usage Guide","text":"<p>This guide provides an in-depth walkthrough of the <code>ibbi</code> package's functionalities. We will cover everything from installation and basic setup to advanced applications like model evaluation and explainability, complete with detailed code examples and explanations to help you integrate <code>ibbi</code> into your research workflow.</p>"},{"location":"usage/#installation","title":"Installation","text":"<p>Getting <code>ibbi</code> set up on your system is a two-step process. Since <code>ibbi</code> relies on PyTorch for its deep learning capabilities, it's crucial to install it first to ensure compatibility with your hardware, especially if you have a GPU.</p>"},{"location":"usage/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li> <p>Disk Space: A minimum of 10-20 GB of disk space is recommended to accommodate the Python environment, downloaded models, and cached datasets.</p> </li> <li> <p>CPU &amp; RAM: Running inference on a CPU is possible but can be slow. For model evaluation on large datasets (like the built-in test set), a significant amount of RAM (16GB, 32GB, or more) is highly recommended to avoid memory crashes.</p> </li> <li> <p>GPU (Recommended): A CUDA-enabled GPU (e.g., NVIDIA T4, RTX 3060 or better) with at least 8GB of VRAM is strongly recommended for both inference and model evaluation.</p> </li> </ul> <p>1. Install PyTorch</p> <p>For optimal performance, particularly with GPU acceleration, it is essential to install the correct version of PyTorch for your system (Windows/Mac/Linux) and CUDA version. Please follow the official, system-specific instructions at pytorch.org. This will ensure you leverage the full power of your hardware.</p> <p>2. Install IBBI</p> <p>Once PyTorch is installed, you can install <code>ibbi</code> from the Python Package Index (PyPI) using pip. It is highly recommended to do this within a virtual environment to avoid conflicts with other projects.</p> \ud83d\udc0d Conda Installation <pre><code># 1. Create and activate a new conda environment\nconda create -n ibbi_env python=3.11 -y\nconda activate ibbi_env\n\n# 2. Install PyTorch first by following the official instructions\n# for your specific system at https://pytorch.org/get-started/locally/\n\n# 3. Install the ibbi package using pip\npip install ibbi\n</code></pre> \ud83d\udfe8 Pixi Installation <pre><code># 1. Initialize a pixi project in your directory (optional)\npixi init\n\n# 2. Install PyTorch first by following the official instructions\n# for your specific system at https://pytorch.org/get-started/locally/\n\n# 3. Add ibbi to your project. Pixi will automatically handle the\n# creation of the environment and resolve all dependencies, including PyTorch.\npixi add --pypi ibbi\n\n# 4. Run commands or scripts within the managed environment\n# For example, to start an interactive python session:\npixi run python\n</code></pre> <p>For developers who want to contribute or use the latest, unreleased features, you can install the package directly from the source on GitHub:</p> <pre><code>pip install git+https://github.com/ChristopherMarais/IBBI.git\n</code></pre>"},{"location":"usage/#core-functions","title":"Core Functions","text":"<p>The <code>ibbi</code> package is designed around a simple and intuitive API. The following core functions are your primary entry points for exploring, loading, and using the models.</p>"},{"location":"usage/#listing-available-models","title":"Listing Available Models","text":"<p>Before you can use a model, you need to know what's available. The <code>ibbi.list_models()</code> function provides a comprehensive overview of all registered models, their tasks, and key performance metrics. This is the best first step to help you select the most appropriate model for your specific needs.</p> <pre><code>import ibbi\n\n# Get the model list as a pandas DataFrame for easy viewing and filtering\nmodels_df = ibbi.list_models(as_df=True)\n\n# Display the full table to see all available models and their metrics\nprint(\"--- Full Model Summary ---\")\nprint(models_df.to_string())\n\n# You can also filter the DataFrame to find models for a specific task\nprint(\"\\n--- Multi-Class Object Detection Models ---\")\nmulti_class_models = models_df[models_df['Tasks'].str.contains(\"Multi-class\")]\nprint(multi_class_models[['Model Name', 'mAP@[.5:.95]', 'F1-score (Macro)']])\n</code></pre>"},{"location":"usage/#creating-a-model","title":"Creating a Model","text":"<p>Once you've chosen a model, you can load it using <code>ibbi.create_model()</code>. The first time you load a model with <code>pretrained=True</code>, its weights will be downloaded from the Hugging Face Hub and stored in a local cache directory (<code>~/.cache/ibbi</code> by default). Subsequent calls will load the model instantly from your local machine.</p> <p>For maximum convenience, <code>ibbi</code> provides simple aliases for the primary recommended model for each major task. This is the easiest way to get started without needing to remember specific model version names.</p> <pre><code># Load a single-class object detection model using the \"beetle_detector\" alias\n# This is ideal for quickly finding any beetle in an image.\ndetector = ibbi.create_model(\"beetle_detector\", pretrained=True)\n\n# Load a multi-class species detection model using its full, specific name\n# This provides both location and species identification.\nclassifier = ibbi.create_model(\"yolov12x_bb_multi_class_detect_model\", pretrained=True)\n\n# Load a zero-shot detection model using the \"zero_shot_detector\" alias\n# This model is great for general-purpose object detection using text prompts.\nzs_detector = ibbi.create_model(\"zero_shot_detector\", pretrained=True)\n\n# Load a feature extraction model using the \"feature_extractor\" alias\n# This is used to convert images into numerical vectors (embeddings).\nfeature_extractor = ibbi.create_model(\"feature_extractor\", pretrained=True)\n</code></pre>"},{"location":"usage/#prediction-and-feature-extraction-examples","title":"Prediction and Feature Extraction Examples","text":"<p>The models can perform inference on a variety of image sources, including a local file path, or a <code>PIL.Image</code> object that is already loaded in your Python script. This flexibility makes it easy to integrate <code>ibbi</code> into diverse data processing workflows.</p> <p>NOTE: We recommend to create different instances of the model for inference and feature extraction to avoid any potential conflicts.</p>"},{"location":"usage/#example-1-bark-beetle-detection-single-class","title":"Example 1: Bark Beetle Detection (Single Class)","text":"<p>Use Case: These models are optimized to answer a simple but crucial question: \"Is there a beetle in this image, and where?\" They identify the location (bounding boxes) of any beetle without identifying the species. This is highly effective for initial screening of field-trap images or other raw collection data to quickly quantify the number of specimens.</p> <p>Input Image:</p> <p> </p> <pre><code>import ibbi\n\ndetector = ibbi.create_model(\"beetle_detector\", pretrained=True)\n\n# You can use a local file path\nimage_source = \"https://media.githubusercontent.com/media/ChristopherMarais/IBBI/main/docs/assets/images/beetles.png\"\n\n# Get bounding box predictions. The model returns a dictionary.\nresults = detector.predict(image_source)\n\n# The result is a dictionary containing the detected boxes, confidence scores, and labels.\nprint(f\"Detected {len(results['boxes'])} beetles.\")\n# Example output for the first detected beetle:\nif results['boxes']:\n    print(f\"  - Box: {results['boxes'][0]}\")\n    print(f\"  - Label: {results['labels'][0]}\")\n    print(f\"  - Score: {results['scores'][0]:.2f}\")\n</code></pre> <p>Detection Output: The output image shows the detected beetles, each enclosed in a red bounding box with its confidence score.</p> <p> </p>"},{"location":"usage/#example-2-species-detection-multi-class","title":"Example 2: Species Detection (Multi-Class)","text":"<p>Use Case: These models perform the core task of detailed biodiversity analysis by simultaneously finding the location of bark beetles and predicting their species. This is invaluable for species inventories, monitoring the spread of invasive species, and conducting ecological research where species-level data is paramount.</p> <pre><code>import ibbi\n\nclassifier = ibbi.create_model(\"species_classifier\", pretrained=True)\n\n# Use the same multi-beetle image to get species-level predictions\nresults = classifier.predict(image_source)\n\n# The output is a dictionary with predicted species labels for each detected box.\nprint(f\"Detected and classified {len(results['boxes'])} beetles.\")\nif results['boxes']:\n    print(f\"  - Top Prediction Box: {results['boxes'][0]}\")\n    print(f\"  - Predicted Species: {results['labels'][0]}\")\n    print(f\"  - Confidence Score: {results['scores'][0]:.2f}\")\n\n</code></pre> <p>Classification Output: The output image now includes species-level labels and confidence scores, allowing for precise identification of each specimen.</p> <p> </p>"},{"location":"usage/#example-3-zero-shot-detection","title":"Example 3: Zero-Shot Detection","text":"<p>Use Case: Zero-shot models offer incredible flexibility by allowing you to detect objects based on a text description, even if the model was not explicitly trained on that class. This is powerful for exploratory analysis, detecting general objects in your samples, or identifying other items of interest like debris or sample labels.</p> <pre><code>import ibbi\n\n# Load the zero-shot detection model\nzs_detector = ibbi.create_model(\"zero_shot_detector\", pretrained=True)\n\n# Predict objects using a text prompt. You can try other prompts too!\n# For example: text_prompt=\"a round object . an insect leg\"\n# Separate different text prompts using a full stop \".\"\nresults = zs_detector.predict(image_source, text_prompt=\"a beetle\")\n\nprint(results)\n</code></pre> <p>Zero-Shot Output: The model successfully identifies objects matching the text prompt, demonstrating its ability to generalize beyond its core training data.</p> <p> </p>"},{"location":"usage/#example-4-feature-extraction","title":"Example 4: Feature Extraction","text":"<p>Use Case: All models can extract feature embeddings for an image. These are dense numerical vectors that capture the complex visual information of an image in a compact form. They are highly useful for advanced downstream tasks where you need to numerically compare images, such as:</p> <ul> <li>Clustering: Grouping visually similar specimens without needing prior labels.</li> <li>Similarity Search: Building a system to find all images in a large dataset that resemble a query image.</li> <li>Training Custom Classifiers: Using the embeddings as powerful input features for simpler machine learning models (like SVMs or Logistic Regression) for specialized tasks.</li> </ul> <p>NOTE: TO get embeddings of specific objects in an image we reccomend using the zero-shot detection or the bark beetle detection models to split the iamge up into sub-iamges first.</p> <pre><code>import ibbi\n\n# Any model can be used, but the dedicated 'feature_extractor' is optimized for this.\nfeature_extractor = ibbi.create_model(\"feature_extractor\", pretrained=True)\n\n# Extract the feature vector from the image\nfeatures = feature_extractor.extract_features(image_source)\n\nprint(f\"Extracted feature vector of shape: {features.shape}\")\n</code></pre>"},{"location":"usage/#advanced-usage","title":"Advanced Usage","text":"<p>Beyond basic inference, <code>ibbi</code> provides powerful tools for rigorously evaluating model performance and understanding their internal decision-making processes. These features are crucial for conducting transparent and reproducible scientific research.</p>"},{"location":"usage/#model-evaluation","title":"Model Evaluation","text":"<p>The <code>ibbi.Evaluator</code> class provides a simple and standardized interface for assessing model performance across different tasks. This is essential for understanding a model's strengths and weaknesses, comparing different models objectively, and reporting robust, quantitative results in publications.</p> <p>\u26a0\ufe0f Important Note on Memory Usage</p> <p>The <code>Evaluator</code> methods (<code>.classification()</code>, <code>.embeddings()</code>) currently process the entire dataset in memory. Attempting to run evaluation on the full test dataset (~2,000 images) at once may exhaust all available RAM and crash your session.</p> <p>To avoid this, we strongly recommend evaluating on a smaller subset of the data, as demonstrated in the code example below (using <code>data.select(range(10))</code>). You can evaluate incrementally over several subsets to build a complete performance picture.</p> <pre><code>import ibbi\n\n# --- 1. Import the test dataset included with the package ---\ndata = ibbi.get_dataset()\n\n# --- Create a small subset for evaluation ---\n# Use .select() to create a new Dataset object, not data[:10]\ndata_subset = data.select(range(10))\n\n# --- 2. Create a model to evaluate ---\nmodel = ibbi.create_model(model_name=\"species_classifier\", pretrained=True)\n\n# --- 3. Instantiate the Evaluator ---\nevaluator = ibbi.Evaluator(model=model)\n\n# --- 4. Run Specific Evaluations ---\n\n# a) Classification Performance: Calculates metrics like accuracy, F1-score, and precision/recall.\n# This tells you how well the model identifies the correct species.\nprint(\"\\\\n--- Classification Performance ---\")\nclassification_results = evaluator.classification(data)\nprint(f\"  Accuracy: {classification_results['accuracy']:.4f}\")\nprint(f\"  Macro F1-Score: {classification_results['macro_f1_score']:.4f}\")\n\n# b) Object Detection Performance: Calculates mean Average Precision (mAP) for bounding box accuracy.\n# This measures how well the model localizes the beetles in the image.\nprint(\"\\\\n--- Object Detection Performance ---\")\nod_results = evaluator.object_detection(data)\nprint(f\"  Mean Average Precision (mAP): {od_results['mAP']:.4f}\")\n\n# c) Embedding Quality: Assesses how well the model's embeddings separate species.\n# This uses clustering metrics and compares embedding distances to a phylogenetic distance matrix.\nprint(\"\\\\n--- Embedding Quality ---\")\nembedding_results = evaluator.embeddings(data)\nprint(f\"  Silhouette Score: {embedding_results['internal_cluster_validation']['Silhouette_Score'].values[0]:.4f}\")\nprint(f\"  Adjusted Rand Index (ARI): {embedding_results['external_cluster_validation']['ARI'].values[0]:.4f}\")\nprint(f\"  Mantel Correlation (r): {embedding_results['mantel_correlation']['r']:.4f}\")\n</code></pre>"},{"location":"usage/#model-explainability-xai","title":"Model Explainability (XAI)","text":"<p>The <code>ibbi.Explainer</code> class provides a simple interface for using popular explainable AI (XAI) techniques like LIME and SHAP. This allows you to look inside the \"black box\" of a deep learning model to understand which parts of an image were most influential in its predictions. This is vital for debugging unexpected predictions, and potentially discovering novel morphological characters that the model uses for identification.</p> <pre><code>import ibbi\n\n# --- 1. Create a Model and Explainer ---\nmodel = ibbi.create_model(model_name=\"species_classifier\", pretrained=True)\nexplainer = ibbi.Explainer(model=model)\nimage_to_explain = \"https://media.githubusercontent.com/media/ChristopherMarais/IBBI/main/docs/assets/images/beetles.png\"\n\n\n# --- 2. Explain with LIME ---\n# LIME is great for a quick, intuitive visualization on a single image.\nprint(\"\\\\n--- Generating LIME Explanation ---\")\nlime_explanation, original_image = explainer.with_lime(image=image_to_explain)\nibbi.plot_lime_explanation(lime_explanation, original_image, top_k=1)\n\n\n# --- 3. Explain with SHAP ---\n# SHAP is more computationally intensive but provides more robust, theoretically-grounded explanations.\n# It requires a \"background\" dataset to simulate the \"absence\" of image features.\nprint(\"\\\\n--- Generating SHAP Explanation ---\")\nbackground_dataset = ibbi.get_shap_background_dataset()\nexplain_dataset = [{\"image\": ibbi.Image.open(image_to_explain)}]\n\nshap_values = explainer.with_shap(\n    explain_dataset=explain_dataset,\n    background_dataset=background_dataset,\n    num_explain_samples=1\n)\n\n# Plot the explanation for the first image\nibbi.plot_shap_explanation(shap_values[0], model)\n</code></pre>"}]}
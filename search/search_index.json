{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Intelligent Bark Beetle Identifier (IBBI)","text":"<p>IBBI is a Python package that provides a simple and unified interface for detecting and classifying bark and ambrosia beetles from images using state-of-the-art computer vision models.</p> <p>This package is designed to support entomological research by automating the laborious task of beetle identification, enabling high-throughput data analysis for ecological studies, pest management, and biodiversity monitoring.</p>"},{"location":"#the-need-for-automation","title":"The Need for Automation","text":"<p>The accurate and timely identification of bark and ambrosia beetle species is fundamental to forest health monitoring. However, many species are morphologically similar, making identification a significant bottleneck that requires highly specialized taxonomic expertise. IBBI addresses this challenge by providing an accessible, programmatic solution that automates the identification process.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Simple API: Access powerful detection and classification models with a single function call: <code>ibbi.create_model()</code>.</li> <li>Multiple Model Types:<ul> <li>Single-Class Detection: Detect the presence of any beetle in an image.</li> <li>Multi-Class Classification: Identify the species of a beetle from a cropped image.</li> <li>Zero-Shot Detection: Detect beetles using a text prompt, without prior training on that specific class.</li> </ul> </li> <li>Pre-trained Models: Leverages pre-trained models hosted on the Hugging Face Hub for immediate use.</li> <li>Model Explainability: Understand model predictions using SHAP (SHapley Additive exPlanations) to visualize which parts of an image contribute to the identification.</li> <li>Extensible: Designed to easily incorporate new model architectures in the future.</li> <li>Research-Focused: Aims to accelerate ecological research by automating beetle identification.</li> </ul>"},{"location":"#workflow-how-the-models-were-built","title":"Workflow: How the Models Were Built","text":"<p>The models in <code>ibbi</code> are trained using a detailed workflow, from data collection and annotation to model evaluation. This process ensures high-quality, reliable models for your research.</p> <p></p>"},{"location":"#package-api-and-usage","title":"Package API and Usage","text":"<p>The <code>ibbi</code> package is designed for ease of use. The main functions, inputs, and outputs are summarized below.</p> <p></p> <p>Ready to get started? Check out the Usage Guide.</p>"},{"location":"CONTRIBUTING/","title":"Contributing to IBBI","text":"<p>First off, thank you for considering contributing to IBBI! Your help is greatly appreciated. This document provides guidelines for contributing to the project.</p>"},{"location":"CONTRIBUTING/#how-can-i-contribute","title":"How Can I Contribute?","text":"<p>There are many ways to contribute, from writing tutorials to implementing new models. Here are a few ideas:</p> <ul> <li>Reporting Bugs: If you find a bug, please open an issue on our GitHub issue tracker. Describe the issue in detail, including steps to reproduce it.</li> <li>Suggesting Enhancements: Have an idea for a new feature or an improvement to an existing one? Open an issue to start a discussion.</li> <li>Writing Documentation: Good documentation is key. If you find parts of our docs unclear or want to add a new tutorial, please let us know or submit a pull request.</li> <li>Adding New Models: If you have trained a new model that would be a good fit for IBBI, we'd love to hear about it.</li> <li>Submitting Pull Requests: If you've fixed a bug or implemented a new feature, you can submit a pull request.</li> </ul>"},{"location":"CONTRIBUTING/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":"<p>To get started with development, please follow these steps.</p> <ol> <li>Clone the repository:     This downloads the project source code to your local machine.</li> </ol> <pre><code>git clone https://github.com/ChristopherMarais/ibbi.git\ncd ibbi\n</code></pre> <ol> <li>Create a Conda environment:     We recommend using Conda to manage your Python environment to avoid conflicts with other projects. This command creates an environment named <code>ibbi</code> with Python 3.11.</li> </ol> <pre><code>conda env create -f environment.yml\nconda activate IBBI\n</code></pre> <ol> <li> <p>Install dependencies with Poetry:     This project uses Poetry for dependency management. The <code>environment.yml</code> file sets up Python and pip, and then we use Poetry to install the project dependencies.</p> </li> <li> <p>Install dependencies with Poetry:     This project uses Poetry for dependency management. These commands will install all the necessary packages for running and developing <code>ibbi</code>.</p> </li> </ol> <pre><code># Install Poetry itself if you don't have it\npip install poetry\n\n# Install PyTorch first, as its installation can be system-specific (CPU/GPU)\n# See https://pytorch.org/get-started/locally/ for the correct command\npip install torch torchvision torchaudio\n\n# Configure Poetry to use the existing Conda environment\npoetry config virtualenvs.create false --local\n\n# Install all other project dependencies, including development tools\npoetry install --with dev\n</code></pre> <ol> <li>Set up pre-commit hooks:     We use <code>pre-commit</code> to automatically run code formatters and linters before each commit. This ensures code quality and a consistent style across the project.</li> </ol> <pre><code>pre-commit install\n</code></pre> <pre><code>The hooks will now run automatically every time you make a commit.\n</code></pre>"},{"location":"CONTRIBUTING/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create a new branch for your feature or bug fix (e.g., <code>git checkout -b feature/my-new-feature</code>).</li> <li>Make your changes and commit them. Make sure your commit messages are clear and descriptive.</li> <li>Ensure all tests pass and that the pre-commit hooks run without errors.</li> <li>Push your branch to your fork on GitHub.</li> <li>Open a pull request from your branch to the <code>main</code> branch of the IBBI repository.</li> <li>In the pull request description, clearly describe the changes you've made and why. If it fixes an existing issue, please reference it (e.g., \"Fixes #123\").</li> </ol> <p>Thank you again for your interest in contributing!</p>"},{"location":"api/","title":"API Reference","text":"<p>This section provides an auto-generated API reference for the <code>ibbi</code> package.</p>"},{"location":"api/#ibbi","title":"<code>ibbi</code>","text":"<p>Main initialization file for the ibbi package.</p>"},{"location":"api/#ibbi.create_model","title":"<code>create_model(model_name, pretrained=False, **kwargs)</code>","text":"<p>Creates a model from a name.</p> <p>This factory function is the main entry point for users of the package. It looks up the requested model in the registry, downloads pretrained weights from the Hugging Face Hub if requested, and returns an instantiated model object.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to create.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load pretrained weights from the Hugging Face Hub.                 Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments to pass to the model-creating function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ModelType</code> <code>ModelType</code> <p>An instance of the requested model (e.g., YOLOSingleClassBeetleDetector or        YOLOBeetleMultiClassDetector).</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the requested <code>model_name</code> is not found in the model registry.</p> Example <pre><code>import ibbi\n\n# Create a pretrained single-class detection model\ndetector = ibbi.create_model(\"yolov10x_bb_detect_model\", pretrained=True)\n\n# Create a pretrained multi-class detection model\nmulti_class_detector = ibbi.create_model(\"yolov10x_bb_multi_class_detect_model\", pretrained=True)\n</code></pre> Source code in <code>src\\ibbi\\__init__.py</code> <pre><code>def create_model(model_name: str, pretrained: bool = False, **kwargs: Any) -&gt; ModelType:\n    \"\"\"\n    Creates a model from a name.\n\n    This factory function is the main entry point for users of the package.\n    It looks up the requested model in the registry, downloads pretrained\n    weights from the Hugging Face Hub if requested, and returns an\n    instantiated model object.\n\n    Args:\n        model_name (str): Name of the model to create.\n        pretrained (bool): Whether to load pretrained weights from the Hugging Face Hub.\n                            Defaults to False.\n        **kwargs (Any): Extra arguments to pass to the model-creating function.\n\n    Returns:\n        ModelType: An instance of the requested model (e.g., YOLOSingleClassBeetleDetector or\n                   YOLOBeetleMultiClassDetector).\n\n    Raises:\n        KeyError: If the requested `model_name` is not found in the model registry.\n\n    Example:\n        ```python\n        import ibbi\n\n        # Create a pretrained single-class detection model\n        detector = ibbi.create_model(\"yolov10x_bb_detect_model\", pretrained=True)\n\n        # Create a pretrained multi-class detection model\n        multi_class_detector = ibbi.create_model(\"yolov10x_bb_multi_class_detect_model\", pretrained=True)\n        ```\n    \"\"\"\n    if model_name not in model_registry:\n        available = \", \".join(model_registry.keys())\n        raise KeyError(f\"Model '{model_name}' not found. Available models: [{available}]\")\n\n    # Look up the factory function in the registry and call it\n    model_factory = model_registry[model_name]\n    model = model_factory(pretrained=pretrained, **kwargs)\n\n    return model\n</code></pre>"},{"location":"api/#ibbi.explain_model","title":"<code>explain_model(model, explain_dataset, background_dataset, num_explain_samples, num_background_samples, max_evals=1000, batch_size=50, image_size=(640, 640), text_prompt=None)</code>","text":"<p>Generates SHAP explanations for a given model. This function is computationally intensive.</p> Source code in <code>src\\ibbi\\xai\\shap.py</code> <pre><code>def explain_model(\n    model: ModelType,\n    explain_dataset: list,\n    background_dataset: list,\n    num_explain_samples: int,\n    num_background_samples: int,\n    max_evals: int = 1000,\n    batch_size: int = 50,\n    image_size: tuple = (640, 640),\n    text_prompt: Optional[str] = None,\n) -&gt; shap.Explanation:\n    \"\"\"\n    Generates SHAP explanations for a given model.\n    This function is computationally intensive.\n    \"\"\"\n    prediction_fn = _prediction_wrapper(model, text_prompt=text_prompt)\n\n    if isinstance(model, GroundingDINOModel):\n        if not text_prompt:\n            raise ValueError(\"A 'text_prompt' is required for explaining a GroundingDINOModel.\")\n        output_names = [text_prompt]\n    else:\n        output_names = model.get_classes()\n\n    background_pil_images = [background_dataset[i][\"image\"].resize(image_size) for i in range(num_background_samples)]\n    background_images = [np.array(img) for img in background_pil_images]\n    background_images_norm = np.stack([_prepare_image_for_shap(img) for img in background_images])\n\n    background_summary = np.median(background_images_norm, axis=0)\n\n    images_to_explain_pil = [explain_dataset[i][\"image\"].resize(image_size) for i in range(num_explain_samples)]\n    images_to_explain = [np.array(img) for img in images_to_explain_pil]\n    images_to_explain_norm = [_prepare_image_for_shap(img) for img in images_to_explain]\n    images_to_explain_array = np.array(images_to_explain_norm)\n\n    masker = ImageMasker(background_summary, shape=images_to_explain_array[0].shape)\n    explainer = shap.Explainer(prediction_fn, masker, output_names=output_names)\n\n    # Ignoring the arg-type error which is due to incorrect type hints in the shap library\n    shap_explanation = explainer(images_to_explain_array, max_evals=max_evals, batch_size=batch_size)  # type: ignore[arg-type]\n    shap_explanation.data = np.array(images_to_explain)\n    return shap_explanation\n</code></pre>"},{"location":"api/#ibbi.get_dataset","title":"<code>get_dataset(repo_id='IBBI-bio/ibbi_test_data', split='train', **kwargs)</code>","text":"<p>Loads a dataset from the Hugging Face Hub.</p> <p>This function is a wrapper around <code>datasets.load_dataset</code> and returns the raw Dataset object, allowing for direct manipulation.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The Hugging Face Hub repository ID of the dataset.              Defaults to \"IBBI-bio/ibbi_test_data\".</p> <code>'IBBI-bio/ibbi_test_data'</code> <code>split</code> <code>str</code> <p>The dataset split to use (e.g., \"train\", \"test\").              Defaults to \"train\".</p> <code>'train'</code> <code>**kwargs</code> <p>Additional keyword arguments passed directly to       <code>datasets.load_dataset</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>The loaded dataset object from the Hugging Face Hub.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the loaded object is not of type <code>Dataset</code>.</p> Example <pre><code>import ibbi\n\n# Load the default test dataset\ntest_data = ibbi.get_dataset()\n\n# Iterate through the first 5 examples\nfor i, example in enumerate(test_data):\n    if i &gt;= 5:\n        break\n    print(example['image'])\n</code></pre> Source code in <code>src\\ibbi\\utils\\data.py</code> <pre><code>def get_dataset(\n    repo_id: str = \"IBBI-bio/ibbi_test_data\",\n    split: str = \"train\",\n    **kwargs,\n) -&gt; Dataset:\n    \"\"\"\n    Loads a dataset from the Hugging Face Hub.\n\n    This function is a wrapper around `datasets.load_dataset` and returns\n    the raw Dataset object, allowing for direct manipulation.\n\n    Args:\n        repo_id (str): The Hugging Face Hub repository ID of the dataset.\n                         Defaults to \"IBBI-bio/ibbi_test_data\".\n        split (str): The dataset split to use (e.g., \"train\", \"test\").\n                         Defaults to \"train\".\n        **kwargs: Additional keyword arguments passed directly to\n                  `datasets.load_dataset`.\n\n    Returns:\n        Dataset: The loaded dataset object from the Hugging Face Hub.\n\n    Raises:\n        TypeError: If the loaded object is not of type `Dataset`.\n\n    Example:\n        ```python\n        import ibbi\n\n        # Load the default test dataset\n        test_data = ibbi.get_dataset()\n\n        # Iterate through the first 5 examples\n        for i, example in enumerate(test_data):\n            if i &gt;= 5:\n                break\n            print(example['image'])\n        ```\n    \"\"\"\n    print(f\"Loading dataset '{repo_id}' (split: '{split}') from Hugging Face Hub...\")\n    try:\n        # Load the dataset from the hub\n        dataset: Union[Dataset, DatasetDict, IterableDataset, IterableDatasetDict] = load_dataset(\n            repo_id, split=split, trust_remote_code=True, **kwargs\n        )\n\n        # Ensure that the returned object is a Dataset\n        if not isinstance(dataset, Dataset):\n            raise TypeError(\n                f\"Expected a 'Dataset' object for split '{split}', but received type '{type(dataset).__name__}'.\"\n            )\n\n        print(\"Dataset loaded successfully.\")\n        return dataset\n    except Exception as e:\n        print(f\"Failed to load dataset '{repo_id}'. Please check the repository ID and your connection.\")\n        raise e\n</code></pre>"},{"location":"api/#ibbi.list_models","title":"<code>list_models(as_df=False)</code>","text":"<p>Displays available models and their key information.</p> <p>Reads the model summary CSV included with the package and prints it. Can also return the data as a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>as_df</code> <code>bool</code> <p>If True, returns the model information as a pandas DataFrame.           If False (default), prints the information to the console.</p> <code>False</code> <p>Returns:</p> Type Description <p>pd.DataFrame or None: A DataFrame if as_df is True, otherwise None.</p> Source code in <code>src\\ibbi\\utils\\info.py</code> <pre><code>def list_models(as_df: bool = False):\n    \"\"\"\n    Displays available models and their key information.\n\n    Reads the model summary CSV included with the package and prints it.\n    Can also return the data as a pandas DataFrame.\n\n    Args:\n        as_df (bool): If True, returns the model information as a pandas DataFrame.\n                      If False (default), prints the information to the console.\n\n    Returns:\n        pd.DataFrame or None: A DataFrame if as_df is True, otherwise None.\n    \"\"\"\n    try:\n        # Find the path to the data file within the package\n        with resources.files(\"ibbi.data\").joinpath(\"ibbi_model_summary.csv\").open(\"r\") as f:\n            df = pd.read_csv(f)\n\n        if as_df:\n            return df\n        else:\n            print(\"Available IBBI Models:\")\n            print(df.to_string())\n\n    except FileNotFoundError:\n        print(\"Error: Model summary file not found.\")\n        return None\n</code></pre>"},{"location":"api/#ibbi.plot_explanations","title":"<code>plot_explanations(shap_explanation_for_single_image, model, top_k=5, text_prompt=None)</code>","text":"<p>Plots SHAP explanations for a SINGLE image.</p> <p>Parameters:</p> Name Type Description Default <code>shap_explanation_for_single_image</code> <code>Explanation</code> <p>A SHAP Explanation object for a SINGLE image.                                To get this, index the output of explain_model (e.g., <code>shap_explanation[0]</code>).</p> required <code>model</code> <code>ModelType</code> <p>The model that was explained.</p> required <code>top_k</code> <code>int</code> <p>The number of top predicted classes to visualize.</p> <code>5</code> <code>text_prompt</code> <code>Optional[str]</code> <p>The text prompt, if a GroundingDINOModel was used.</p> <code>None</code> Source code in <code>src\\ibbi\\xai\\shap.py</code> <pre><code>def plot_explanations(\n    shap_explanation_for_single_image: shap.Explanation,\n    model: ModelType,\n    top_k: int = 5,\n    text_prompt: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Plots SHAP explanations for a SINGLE image.\n\n    Args:\n        shap_explanation_for_single_image: A SHAP Explanation object for a SINGLE image.\n                                           To get this, index the output of explain_model (e.g., `shap_explanation[0]`).\n        model: The model that was explained.\n        top_k: The number of top predicted classes to visualize.\n        text_prompt: The text prompt, if a GroundingDINOModel was used.\n    \"\"\"\n    print(\"\\n--- Generating Explanations for Image ---\")\n\n    image_for_plotting = shap_explanation_for_single_image.data\n    shap_values = shap_explanation_for_single_image.values\n    class_names = np.array(shap_explanation_for_single_image.output_names)\n\n    prediction_fn = _prediction_wrapper(model, text_prompt=text_prompt)\n    image_norm = _prepare_image_for_shap(np.array(image_for_plotting))\n    prediction_scores_batch = prediction_fn(np.expand_dims(image_norm, axis=0))\n    prediction_scores = prediction_scores_batch[0]\n\n    top_indices = np.argsort(prediction_scores)[-top_k:][::-1]\n\n    plt.figure(figsize=(5, 5))\n    plt.imshow(image_for_plotting)\n    plt.title(\"Original Image\")\n    plt.axis(\"off\")\n    plt.show()\n\n    for class_idx in top_indices:\n        if prediction_scores[class_idx] &gt; 0:\n            class_name = class_names[class_idx]\n            score = prediction_scores[class_idx]\n            print(f\"Explanation for '{class_name}' (Prediction Score: {score:.3f})\")\n\n            # FIX 2: Added `# type: ignore` to suppress the incorrect slicing error from pyright.\n            # The slicing logic is correct for the shape of the shap_values array.\n            shap_values_for_class = shap_values[:, :, :, class_idx]  # type: ignore[misc]\n\n            shap.image_plot(\n                shap_values=[shap_values_for_class],\n                pixel_values=np.array(image_for_plotting),\n                show=True,\n            )\n</code></pre>"},{"location":"usage/","title":"Usage Guide","text":"<p>This guide will walk you through the primary functionalities of the <code>ibbi</code> package, from installation to prediction and feature extraction.</p>"},{"location":"usage/#installation","title":"Installation","text":"<p>First, ensure you have PyTorch installed according to the official instructions at pytorch.org. Then, install <code>ibbi</code> from PyPI:</p> <pre><code>pip install ibbi\n</code></pre>"},{"location":"usage/#core-functions","title":"Core Functions","text":"<p>The <code>ibbi</code> package is designed for simplicity. Here are the core functions you will use.</p>"},{"location":"usage/#listing-available-models","title":"Listing Available Models","text":"<p>To see all available models and their performance metrics, use <code>ibbi.list_models()</code>. This is useful for choosing the best model for your needs.</p> <pre><code>import ibbi\n\n# Get the model list as a pandas DataFrame\nmodels_df = ibbi.list_models(as_df=True)\nprint(models_df)\n</code></pre>"},{"location":"usage/#creating-a-model","title":"Creating a Model","text":"<p>Load any model from the list using <code>ibbi.create_model()</code> by passing its name.</p> <pre><code># Load a single-class object detection model\ndetector = ibbi.create_model(\"yolov10x_bb_detect_model\", pretrained=True)\n\n# Load a multi-class species detection model\nclassifier = ibbi.create_model(\"yolov10x_bb_multi_class_detect_model\", pretrained=True)\n\n# Load a zero-shot detection model\nzs_detector = ibbi.create_model(\"grounding_dino_detect_model\", pretrained=True)\n</code></pre>"},{"location":"usage/#prediction-examples","title":"Prediction Examples","text":"<p>You can perform inference on images from a file path, a URL, or a PIL Image object.</p>"},{"location":"usage/#example-1-bark-beetle-detection-single-class","title":"Example 1: Bark Beetle Detection (Single Class)","text":"<p>These models find the location (bounding boxes) of any beetle in an image, without identifying the species.</p> <p>Input Image: </p> <pre><code>import ibbi\nfrom PIL import Image\n\ndetector = ibbi.create_model(\"yolov10x_bb_detect_model\", pretrained=True)\n\n# Use an image URL or local path\nimage_source = \"[https://raw.githubusercontent.com/christopher-marais/IBBI/main/docs/assets/images/beetles.png](https://raw.githubusercontent.com/christopher-marais/IBBI/main/docs/assets/images/beetles.png)\"\n\n# Get bounding box predictions\nresults = detector.predict(image_source)\n\n# The 'results' object contains bounding boxes, scores, and labels.\n# To display the image with bounding boxes:\nresults[0].show()\n</code></pre> <p>Detection Output: </p>"},{"location":"usage/#example-2-species-detection-multi-class","title":"Example 2: Species Detection (Multi-Class)","text":"<p>These models simultaneously find the location of beetles and predict their species.</p> <pre><code>classifier = ibbi.create_model(\"yolov10x_bb_multi_class_detect_model\", pretrained=True)\n\n# Use the same multi-beetle image\nresults = classifier.predict(image_source)\n\n# The 'results' object contains bounding boxes with predicted species.\n# To see the top prediction for the first detected beetle:\nfirst_box = results[0].boxes[0]\npredicted_species_index = int(first_box.cls)\npredicted_species = results[0].names[predicted_species_index]\nconfidence = float(first_box.conf)\n\nprint(f\"Predicted Species: {predicted_species} with confidence: {confidence:.2f}\")\n\n# To display the image with bounding boxes and class labels:\nresults[0].show()\n</code></pre> <p>Classification Output: </p>"},{"location":"usage/#example-3-zero-shot-detection","title":"Example 3: Zero-Shot Detection","text":"<p>Zero-shot models can detect objects based on a text description, without being explicitly trained on that class. This is powerful for detecting objects not in the training data.</p> <pre><code># Load the zero-shot detection model\nzs_detector = ibbi.create_model(\"grounding_dino_detect_model\", pretrained=True)\n\n# Predict with a text prompt\nresults = zs_detector.predict(image_source, text_prompt=\"a beetle\")\nresults[0].show()\n</code></pre> <p>Zero-Shot Output: </p>"},{"location":"usage/#advanced-usage","title":"Advanced Usage","text":""},{"location":"usage/#feature-extraction","title":"Feature Extraction","text":"<p>All models can extract deep feature embeddings from an image. These vectors are useful for downstream tasks like clustering, similarity analysis, or training other machine learning models.</p> <pre><code># Assuming 'classifier' is a loaded model\nfeatures = classifier.extract_features(image_source)\n\nprint(f\"Extracted feature vector shape: {features.shape}\")\n</code></pre>"},{"location":"usage/#model-explainability-with-shap","title":"Model Explainability with SHAP","text":"<p>Understand why a model made a certain prediction using SHAP (SHapley Additive exPlanations). This is crucial for building trust and interpreting the model's decisions by highlighting which pixels were most influential.</p> <pre><code>import ibbi\n\n# Load a model\nmodel = ibbi.create_model(\"yolov10x_bb_multi_class_detect_model\", pretrained=True)\n\n# Get a few images to explain and a background dataset\n# Note: Using more images for background_dataset provides better explanations\nexplain_data = ibbi.get_dataset(split=\"train\", streaming=True).take(5)\nbackground_data = ibbi.get_dataset(split=\"train\", streaming=True).skip(5).take(10)\n\n# Generate explanations (this is computationally intensive)\nshap_explanation = ibbi.explain_model(\n    model=model,\n    explain_dataset=list(explain_data),\n    background_dataset=list(background_data),\n    num_explain_samples=1, # Number of images to explain\n    num_background_samples=5 # Number of background images to use\n)\n\n# Plot the explanation for the first image\nibbi.plot_explanations(shap_explanation[0], model)\n</code></pre>"},{"location":"usage/#loading-the-dataset","title":"Loading the Dataset","text":"<p>The dataset used to train and evaluate the models can be loaded for your own research and validation.</p> <p>```python import ibbi</p>"},{"location":"usage/#load-the-dataset-it-will-be-downloaded-and-cached-locally","title":"Load the dataset (it will be downloaded and cached locally)","text":""},{"location":"usage/#set-streamingfalse-to-download-the-full-dataset","title":"Set streaming=False to download the full dataset","text":"<p>dataset = ibbi.get_dataset(split=\"test\", streaming=False) print(f\"Dataset loaded: {dataset}\")</p>"},{"location":"usage/#you-can-also-iterate-through-it-without-downloading-everything","title":"You can also iterate through it without downloading everything","text":"<p>streaming_dataset = ibbi.get_dataset(split=\"train\", streaming=True) print(next(iter(streaming_dataset)))</p>"}]}